{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "3eQD_OhqmeeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855470c2-1ea4-4717-8eab-4aac9286d1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "import tqdm\n",
        "import pprint\n",
        "\n",
        "# in a notebook, load the tensorboard extension, not needed for scripts\n",
        "%load_ext tensorboard\n",
        "\n",
        "#load the mnist dataset\n",
        "(train_ds, test_ds) = tfds.load('Cifar10', split=['train', 'test'], as_supervised=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(batch_size, data): \n",
        "  '''\n",
        "  prepare the dataset to have one-hot-vectors and values between -1 and 1\n",
        "  '''\n",
        "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), tf.cast(target,tf.int32)))\n",
        "  data = data.map(lambda img, target: ((img/128.)-1.0, target))\n",
        "  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "  \n",
        "  data = data.cache()\n",
        "  data = data.shuffle(1000)\n",
        "  data = data.batch(batch_size)\n",
        "  data = data.prefetch(20)\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "cu6QEg08F9oO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class BasicConv(tf.keras.Model):\n",
        "  '''\n",
        "  create a small network with 2 blocks each having 2 layers and starting with 32 filters in the first layer\n",
        "  '''\n",
        "  def __init__(self,optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(BasicConv, self).__init__()\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    self.metrics_list = [ tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    ]\n",
        "\n",
        "    #original input size is 32*32, we have 32 as number of filters\n",
        "    self.l1 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l2 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    #after pooling the size is reduced to 16*16\n",
        "    self.l3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l4 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "\n",
        "    # last layer is a dense layer with softmax activation \n",
        "    self.out = tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "\n",
        "  def call(self, x): \n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "    x = self.global_pool(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}"
      ],
      "metadata": {
        "id": "NlczHtedKiWy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class BasicConvFilter1(tf.keras.Model):\n",
        "  '''\n",
        "  create a small network with 2 blocks each having 2 layers and starting with 64 filters in the first layer\n",
        "  '''\n",
        "  def __init__(self,optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(BasicConvFilter1, self).__init__()\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    self.metrics_list = [ tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    ]\n",
        "\n",
        "    #original input size is 32*32, we have 32 as number of filters\n",
        "    self.l1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l2 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    #after pooling the size is reduced to 16*16\n",
        "    self.l3 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l4 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "\n",
        "    # last layer is a dense layer with softmax activation \n",
        "    self.out = tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "\n",
        "  def call(self, x): \n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "    x = self.global_pool(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}"
      ],
      "metadata": {
        "id": "crXguDQtKt_w"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class BasicConvFilter2(tf.keras.Model):\n",
        "  '''\n",
        "  create a small network with 2 blocks each having 2 layers and starting with 16 filters in the first layer\n",
        "  '''\n",
        "  def __init__(self,optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(BasicConvFilter2, self).__init__()\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    self.metrics_list = [ tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    ]\n",
        "\n",
        "    #original input size is 32*32, we have 32 as number of filters\n",
        "    self.l1 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l2 = tf.keras.layers.Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    #after pooling the size is reduced to 16*16\n",
        "    self.l3 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l4 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "\n",
        "    # last layer is a dense layer with softmax activation \n",
        "    self.out = tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "\n",
        "  def call(self, x): \n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "    x = self.global_pool(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}"
      ],
      "metadata": {
        "id": "sxvgB-FqK4T1"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class BasicConvDeep(tf.keras.Model):\n",
        "  '''\n",
        "  create a deeper network with 4 blocks each having 2 layers and starting with 32 filters in the first layer\n",
        "  '''\n",
        "\n",
        "  def __init__(self,optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(BasicConvDeep, self).__init__()\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    self.metrics_list = [ tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    ]\n",
        "\n",
        "    #original input size is 32*32, we have 32 as number of filters\n",
        "    self.l1 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l2 = tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    #after pooling the size is reduced to 16*16\n",
        "    self.l3 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l4 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.pooling1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    self.l5 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l6 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    self.l7 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "    self.l8 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu')\n",
        "\n",
        "    self.global_pool = tf.keras.layers.GlobalAvgPool2D()\n",
        "\n",
        "    # last layer is a dense layer with softmax activation \n",
        "    self.out = tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "\n",
        "  def call(self, x): \n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.l3(x)\n",
        "    x = self.l4(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.l5(x)\n",
        "    x = self.l6(x)\n",
        "    x = self.pooling2(x)\n",
        "    x = self.l7(x)\n",
        "    x = self.l8(x)\n",
        "    x = self.global_pool(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}"
      ],
      "metadata": {
        "id": "dUQ1Kr1dAPXP"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenselyConnectedCNNLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filter): \n",
        "    super(DenselyConnectedCNNLayer, self).__init__()\n",
        "\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = num_filter, kernel_size = 3, padding = \"same\", activation = \"relu\")\n",
        "\n",
        "  def call(self,x): \n",
        "    c = self.conv(x)\n",
        "    #concatenate at last layer - get long stack of all earlier layer\n",
        "    x = tf.concat((x,c), axis = -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DenselyConnectedCNNBlock(tf.keras.layers.Layer):\n",
        "  #take all conv layers and create a block we take for one convolution\n",
        "  def __init__(self, num_filters, layers):\n",
        "    super(DenselyConnectedCNNBlock, self).__init__()\n",
        "    self.layers = [DenselyConnectedCNNLayer(num_filters) for _ in range(layers)]\n",
        "\n",
        "  def call(self, x): \n",
        "    #take input and pump it through all layers we have \n",
        "    for l in self.layers:\n",
        "      x = l(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "class DenselyConnectedCNN(tf.keras.Model):\n",
        "  def __init__(self, optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(DenselyConnectedCNN, self).__init__()\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    self.metrics_list = [ tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    ]\n",
        "\n",
        "    #4 layers with each of 24 kernels -> get 4 layer in 28*28 resolution \n",
        "    self.denseblock1 = DenselyConnectedCNNBlock(24,4)\n",
        "    #go down to 14*14\n",
        "    self.pooling1 = tf.keras.layers.MaxPooling2D()\n",
        "\n",
        "    self.denseblock2 = DenselyConnectedCNNBlock(24,4)\n",
        "    self.pooling2 = tf.keras.layers.MaxPooling2D()\n",
        "\n",
        "    self.denseblock3 = DenselyConnectedCNNBlock(24,4)\n",
        "    self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
        "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.denseblock1(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.denseblock2(x)\n",
        "    x = self.pooling2(x)\n",
        "    x = self.denseblock3(x)\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "\n",
        "dense_model = DenselyConnectedCNN()\n"
      ],
      "metadata": {
        "id": "nsXHvAwbDD3J"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnectedCNNLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filters, optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(ResidualConnectedCNNLayer, self).__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=3, padding='same', activation='relu')\n",
        "\n",
        "  def call(self, x):\n",
        "    c = self.conv(x)\n",
        "    #residual connection \n",
        "    x = c+x\n",
        "    return x\n",
        "\n",
        "class ResidualConnectedCNNBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, depth, layers):\n",
        "    super(ResidualConnectedCNNBlock, self).__init__()\n",
        "    #depth increases in the end of the network (cannot add them with different sizes)\n",
        "    #specify the depth such that they get the same depth \n",
        "    self.deeper_layer = tf.keras.layers.Conv2D(filters=depth, kernel_size=3, padding='same', activation='relu')\n",
        "    #list with many residual connected cnn layer\n",
        "    self.layers = [ResidualConnectedCNNLayer(depth) for _ in range(layers)]\n",
        "\n",
        "  def call(self, x):\n",
        "    #bring it to the correct depth \n",
        "    x = self.deeper_layer(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class ResidualConnectedCNN(tf.keras.Model):\n",
        "  def __init__(self, optimizer = tf.keras.optimizers.Adam()):\n",
        "    super(ResidualConnectedCNN, self).__init__()\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    self.metrics_list = [ tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    ]\n",
        "    self.residualblock1 = ResidualConnectedCNNBlock(24,4)\n",
        "    self.pooling1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    self.residualblock2 = ResidualConnectedCNNBlock(48,4)\n",
        "    self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "\n",
        "    self.residualblock3 = ResidualConnectedCNNBlock(96,4)\n",
        "    self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
        "    \n",
        "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.residualblock1(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.residualblock2(x)\n",
        "    x = self.pooling2(x)\n",
        "    x = self.residualblock3(x)\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "\n",
        "resnet = ResidualConnectedCNN()"
      ],
      "metadata": {
        "id": "tPL0jxQhDA9K"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer): \n",
        "  '''\n",
        "  create the training loop where the model is trained on the train dataset and afterwards tested on the validation dataset\n",
        "  '''\n",
        "  #iterate over given amount of epochs\n",
        "  for epoch in range(epochs): \n",
        "    print(f\"Epoch {epoch}: \")\n",
        "\n",
        "    #train on all batches of the training data\n",
        "    for data in tqdm.tqdm(train_ds, position = 0, leave = True):\n",
        "\n",
        "      metrics = model.train_step(data)\n",
        "\n",
        "\n",
        "      with train_summary_writer.as_default(): \n",
        "        for metric in model.metrics: \n",
        "          tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "    # print the metrics\n",
        "    print([f\"Train {key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    #reset metric \n",
        "    model.reset_metrics()\n",
        "  \n",
        "    #evaluation on validation set\n",
        "    for data in val_ds:\n",
        "      metrics = model.test_step(data)\n",
        "\n",
        "      with val_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "          tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "      \n",
        "    # print the metrics\n",
        "    print([f\"Test {key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    #reset metric\n",
        "    model.reset_metric()\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "AI2WJpkkMhEO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_summary_writers(config_name):\n",
        "  '''\n",
        "  create the summary writer to have access to the metrics of the model \n",
        "  '''\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  train_log_path = f\"logs/submission/{config_name}/{current_time}/train\"\n",
        "  val_log_path = f\"logs/submission/{config_name}/{current_time}/val\"\n",
        "\n",
        "  # log writer\n",
        "  train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "  val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "    \n",
        "  return train_summary_writer, val_summary_writer"
      ],
      "metadata": {
        "id": "4qLXtIkAFun0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "def run(model):\n",
        "  '''\n",
        "  run the model and specify the number of epochs, batch_size and prepare the training and testing data\n",
        "  '''\n",
        "\n",
        "  ### Hyperparameters\n",
        "  num_epochs = 10\n",
        "  batch_size = 32\n",
        "  train = prepare_data(batch_size,train_ds)\n",
        "  test = prepare_data(batch_size,test_ds)\n",
        "\n",
        "  train_summary_writer, val_summary_writer = create_summary_writers(config_name = f'RUN')\n",
        "  training_loop(model,train,test,num_epochs,train_summary_writer, val_summary_writer)\n",
        "  # print(train_summary_writer)\n",
        "\n",
        "  # #create two different plots for losses and accuracies\n",
        "  # fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "  # #plot the losses\n",
        "  # ax[0].plot(train_summary_writer.scalar(), \"b-\", label = \"train loss\")\n",
        "  # ax[0].plot(val_summary_writer.scalar(), \"r-\", label = \"test_loss\")\n",
        "  # ax[0].legend(loc = \"best\")\n",
        "  # ax[0].set_xlabel(\"Training steps\")\n",
        "  # ax[0].set_ylabel(\"Loss performance\")\n",
        "\n",
        "  \n",
        "  # #move the plot to the right (to not have the label inside the ax[0] plot)\n",
        "  # # box = ax[1].get_position() \n",
        "  # # box.x0 = box.x0 + 0.05\n",
        "  # # box.x1 = box.x1 + 0.05\n",
        "  # # ax[1].set_position(box)\n",
        "\n",
        "  # #set the title \n",
        "  # fig.suptitle(\"Performance of the model\")\n",
        "\n",
        "  # plt.show()\n"
      ],
      "metadata": {
        "id": "yh-7_Bnt8xKR"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMod1 = BasicConv(tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9))\n",
        "\n",
        "myMod2 = BasicConv(tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "myMod3 = BasicConv(tf.keras.optimizers.RMSprop(learning_rate=0.001))\n",
        "myMod4 = BasicConv(tf.keras.optimizers.Adagrad(learning_rate=0.001))\n",
        "myMod5 = BasicConv(tf.keras.optimizers.SGD(learning_rate=0.001))\n",
        "\n",
        "print(\"BasicConv, optimizer=SGD, learning_rate=0.001, momentum=0.9\\n\")\n",
        "run(myMod1)\n",
        "print(\"\\nBasicConv, optimizer=Adam, learning_rate=0.001\\n\")\n",
        "run(myMod2)\n",
        "print(\"BasicConv, optimizer=RMSProp, learning_rate=0.001\\n\")\n",
        "run(myMod3)\n",
        "print(\"\\nBasicConv, optimizer=Adagrad, learning_rate=0.001\\n\")\n",
        "run(myMod4)\n",
        "print(\"\\BasicConv, optimizer=SGD, learning_rate=0.001\\n\")\n",
        "run(myMod5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MMWSPGqHPgM",
        "outputId": "1ded9d4a-0fd6-44b1-df66-13f465b22448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BasicConv, optimizer=SGD, learning_rate=0.001, momentum=0.9\n",
            "\n",
            "Epoch 0: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [03:21<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 2.142871379852295', 'Train acc: 0.191880002617836']\n",
            "['Test loss: 2.0004630088806152', 'Test acc: 0.2551000118255615']\n",
            "\n",
            "\n",
            "Epoch 1: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:50<00:00,  9.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.9060688018798828', 'Train acc: 0.2815200090408325']\n",
            "['Test loss: 1.793137788772583', 'Test acc: 0.33500000834465027']\n",
            "\n",
            "\n",
            "Epoch 2: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [02:51<00:00,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.7497583627700806', 'Train acc: 0.3366200029850006']\n",
            "['Test loss: 1.698600172996521', 'Test acc: 0.36579999327659607']\n",
            "\n",
            "\n",
            "Epoch 3: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 877/1563 [01:36<01:14,  9.18it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myMod6 = BasicConvFilter1(tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9))\n",
        "myMod7 = BasicConvFilter1(tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "print(\"BasicConvFilter1 with a higher number of filters, optimizer=SGD, learning_rate=0.001, momentum=0.9\\n\")\n",
        "run(myMod6)\n",
        "print(\"\\nBasicConvFilter1 with a higher number of filters, optimizer=Adam, learning_rate=0.001\\n\")\n",
        "run(myMod7)"
      ],
      "metadata": {
        "id": "BkC5lhTfL10p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMod8 = BasicConvFilter2(tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9))\n",
        "myMod9 = BasicConvFilter2(tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "print(\"BasicConvFilter2 with a lower number of filters, optimizer=SGD, learning_rate=0.001, momentum=0.9\\n\")\n",
        "run(myMod8)\n",
        "print(\"\\nBasicConvFilter2 with a lower number of filters, optimizer=Adam, learning_rate=0.001\\n\")\n",
        "run(myMod9)"
      ],
      "metadata": {
        "id": "HqB5jwncNQ3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMod10 = BasicConvDeep(tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9))\n",
        "myMod11 = BasicConvDeep(tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "print(\"BasicConvDeep with a deeper network, optimizer=SGD, learning_rate=0.001, momentum=0.9\\n\")\n",
        "run(myMod10)\n",
        "print(\"\\nBasicConvDeep with a deeper network, optimizer=Adam, learning_rate=0.001\\n\")\n",
        "run(myMod11)"
      ],
      "metadata": {
        "id": "r56NEqYQNTR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMod12 = ResidualConnectedCNN(tf.keras.optimizers.SGD(learning_rate=0.001))\n",
        "\n",
        "print(\"Residual Convolutional Network, optimizer=SGD, learning_rate=0.001\")\n",
        "run(myMod12)"
      ],
      "metadata": {
        "id": "5NNPqRX7NU36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMod13 = DenselyConnectedCNN(tf.keras.optimizers.SGD(learning_rate=0.001))\n",
        "\n",
        "print(\"Densely Connected Convolutional Network, optimizer=SGD\\n\")\n",
        "x = run(myMod13)\n",
        "print(x)\n"
      ],
      "metadata": {
        "id": "cr0LlXEjNYkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/submission/"
      ],
      "metadata": {
        "id": "tUQwXJh3E0PP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}