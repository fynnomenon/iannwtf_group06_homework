{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7HSdnSt3ejLn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:52:48.402170: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-04 17:52:48.592541: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-04 17:52:48.592580: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-04 17:52:49.582721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-04 17:52:49.582803: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-04 17:52:49.582811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@authors: faurand, chardes, ehagensieker\n",
    "\"\"\"\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heUMdMV3fY6T",
    "outputId": "79b52605-963c-42ef-971f-98148f80e0fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "\n",
      "1:5 And God called the light Day, and the darkness he called Night.\n",
      "And the evening and the mornin\n"
     ]
    }
   ],
   "source": [
    "# Open the dataset\n",
    "with open(\"bible.txt\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "# print a short example\n",
    "print(txt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcU151XIRA-e",
    "outputId": "69e44d52-dd88-4305-a36d-c65476228ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted: the first book of moses called genesis in the beginning god created the heaven and the earth and the \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:52:52.554266: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-04 17:52:52.554300: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-04 17:52:52.554325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n",
      "2023-02-04 17:52:52.554632: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: ['the', 'first', 'book', 'of', 'moses', 'called', 'genesis', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth', 'and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', 'and', 'the', 'spirit', 'of', 'god', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', 'and', 'god', 'said', 'let']\n"
     ]
    }
   ],
   "source": [
    "# convert to lower case, remove \\n and special characters\n",
    "txt = txt.replace(\"\\n\", \" \").lower()\n",
    "txt = re.sub('[^A-Za-z]+', ' ', txt) \n",
    "print(f\"converted: {txt[:100]} \\n\")\n",
    "\n",
    "# tokenize the text\n",
    "txt = tf_text.WhitespaceTokenizer().split(txt)\n",
    "txt = list(txt.numpy().astype('U'))\n",
    "print(f\"tokenized: {txt[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNXubl-x8oA1",
    "outputId": "9269d31b-e65f-4f9c-a3e4-079269618557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 most common words:  ['the', 'and', 'of', 'to', 'that', 'in', 'he', 'shall', 'unto', 'for', 'i', 'his', 'a', 'lord', 'they', 'be', 'is', 'him', 'not', 'them', 'it', 'with', 'all', 'thou', 'thy', 'was', 'god', 'which', 'my', 'me', 'said', 'but', 'ye', 'their', 'have', 'will', 'thee', 'from', 'as', 'are', 'when', 'this', 'out', 'were', 'upon', 'man', 'by', 'you', 'israel', 'king']\n"
     ]
    }
   ],
   "source": [
    "# counting all words from the corpus and get 10000 most frequent words\n",
    "count = Counter(txt).most_common(10000)\n",
    "words = [x[0] for x in count]\n",
    "print(\"The 20 most common words: \", words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 216, 401, 2, 132, 160, 8616, 5, 0, 679, 26, 1295, 0, 170, 1, 0, 111, 1, 0, 111, 25, 220, 1999, 1, 2000, 1, 497, 25, 44, 0, 227, 2, 0, 971, 1, 0, 189, 2, 26, 867, 44, 0, 227, 2, 0, 304, 1, 26, 30, 79]\n",
      "789262\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to save mappings from tokens to integer indices\n",
    "vocab = {token: idx for idx, token in enumerate(words)}\n",
    "# create another dictionary to store the inverse mapping\n",
    "inverse_vocab = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "# convert the words into integers\n",
    "txt = [vocab[word] for word in txt if word in vocab.keys()]\n",
    "print(txt[:50])\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-E-8Rjpnmgo",
    "outputId": "62791cec-4ed9-4101-a105-27a88c136d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[216, 401, 2, 132, 160, 8616, 679, 26, 1295, 170, 111, 111, 220, 1999, 2000, 497, 25, 44, 227, 971, 189, 867, 44, 227, 304, 26, 30, 79, 52, 15, 326, 52, 25, 326, 26, 177, 326, 20, 147, 26, 930, 0, 326, 497, 160, 0, 326, 70, 497, 160]\n",
      "526163\n"
     ]
    }
   ],
   "source": [
    "s = 0.001\n",
    "word_counts = Counter(txt)\n",
    "total_count = len(txt)\n",
    "\n",
    "# calculating for each word the fraction of the total words being this specific word\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "# calculating for each word the probability of keeping this specific word\n",
    "p_keep = {word: ((np.sqrt(freqs[word]/s) + 1)*s/freqs[word]) for word in word_counts}\n",
    "\n",
    "# apply subsampling to discard words that appear very often\n",
    "txt = [word for word in txt if random.random() < p_keep[word]]\n",
    "print(txt[:50])\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0zH27Q48oKu",
    "outputId": "3926754f-160c-4521-bfa3-41863a860a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: first, target word: book \n",
      "\n",
      "input word: first, target word: of \n",
      "\n",
      "input word: book, target word: first \n",
      "\n",
      "input word: book, target word: of \n",
      "\n",
      "input word: book, target word: moses \n",
      "\n",
      "input word: of, target word: first \n",
      "\n",
      "input word: of, target word: book \n",
      "\n",
      "input word: of, target word: moses \n",
      "\n",
      "input word: of, target word: called \n",
      "\n",
      "input word: moses, target word: book \n",
      "\n",
      "input word: moses, target word: of \n",
      "\n",
      "input word: moses, target word: called \n",
      "\n",
      "input word: moses, target word: genesis \n",
      "\n",
      "input word: called, target word: of \n",
      "\n",
      "input word: called, target word: moses \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the input-target pairs\n",
    "pairs = []\n",
    "for i in range(len(txt)):\n",
    "    for j in [-2, -1, 1, 2]: # window size of 4\n",
    "        if i + j >= 0 and i + j < len(txt):\n",
    "            pairs.append((txt[i], txt[i + j]))\n",
    "\n",
    "for pair in pairs[:15]:\n",
    "    print(f\"input word: {inverse_vocab[pair[0]]}, target word: {inverse_vocab[pair[1]]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 128] \n",
      " ([0 44 150 ... 109 79 37], [930 111 2475 ... 70 1 326])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:52:57.223033: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the data set\n",
    "pairs = np.array(pairs)\n",
    "ds = tf.data.Dataset.from_tensor_slices((pairs[:,0], pairs[:,1]))\n",
    "\n",
    "# shuffle, batch, prefetch and cache\n",
    "ds = ds.shuffle(1024).batch(128, drop_remainder=True).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "# investigate the newly created data set\n",
    "for batch in ds.take(1):\n",
    "    tf.print(tf.shape(batch), \"\\n\", batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8K_jBBoORmIP"
   },
   "outputs": [],
   "source": [
    " class SkipGramModel(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, optimizer, embedding_size, vocabulary_size, counts, num_negative_samples=64):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.optimizer = optimizer\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.counts = counts\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
    "        \n",
    "        # Neural network weights and biases\n",
    "        self.nce_weights = tf.Variable(tf.random.truncated_normal([self.vocabulary_size, self.embedding_size], stddev=0.1 / np.sqrt(self.embedding_size)))\n",
    "        self.nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = tf.Variable(tf.random.uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "\n",
    "    def call(self, input_words):\n",
    "        # Look up embeddings for a batch of inputs\n",
    "        embed = tf.nn.embedding_lookup(self.embedding, input_words)\n",
    "    \n",
    "        return embed\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "    \n",
    "    def get_nce_loss(self, target_words, input_words, embed):\n",
    "        # Sample a number of negative samples for given input words\n",
    "        sampled_values = tf.random.fixed_unigram_candidate_sampler(true_classes=tf.reshape(input_words, (128,1)),\n",
    "                                                                   num_true=1,\n",
    "                                                                   num_sampled=self.num_negative_samples,\n",
    "                                                                   unique=True,\n",
    "                                                                   range_max=self.vocabulary_size,\n",
    "                                                                   unigrams=self.counts,\n",
    "                                                                   name=\"negative_sampling\")\n",
    "        \n",
    "        # Compute the noise contrastive loss\n",
    "        nce_loss = tf.nn.nce_loss(weights =self.nce_weights,\n",
    "                                  biases=self.nce_biases,\n",
    "                                  labels=target_words, \n",
    "                                  inputs=embed,  \n",
    "                                  num_sampled=self.num_negative_samples, \n",
    "                                  num_classes=self.vocabulary_size,\n",
    "                                  sampled_values=sampled_values)\n",
    "        \n",
    "        return tf.reduce_mean(nce_loss)\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, data):\n",
    "        input_words, target_words = data\n",
    "      \n",
    "        with tf.GradientTape() as tape:\n",
    "            # get the embedding\n",
    "            embed = self(input_words)\n",
    "            # compute the loss\n",
    "            loss = self.get_nce_loss(target_words, input_words, embed)\n",
    "        \n",
    "        # compute the gradients and update the network\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update the metrics \n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def validate(self, val_ds):\n",
    "        # Compute the cosine similarity between a few common words and all embeddings\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(self.embedding), 1, keepdims=True))\n",
    "        normalized_embedding = self.embedding / norm\n",
    "        val_embedding = tf.nn.embedding_lookup(normalized_embedding, val_ds)\n",
    "        sim = tf.matmul(val_embedding, tf.transpose(normalized_embedding))\n",
    "        \n",
    "        return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writer(config_name):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    log_path = f\"logs/skipgram/{config_name}/{current_time}\"\n",
    "    summary_writer = tf.summary.create_file_writer(log_path)\n",
    "\n",
    "    return summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DEGUCTSUGlV3"
   },
   "outputs": [],
   "source": [
    "def training_loop(ds, model, epochs, val_ds, val_words, summary_writer): \n",
    "    # iterate over given amount of epochs\n",
    "    for epoch in range(epochs):     \n",
    "        print(f\"Epoch {epoch}: \")\n",
    "\n",
    "        for data in tqdm.tqdm(ds, position = 0, leave = True):\n",
    "            # get the loss\n",
    "            metrics = model.train(data)\n",
    "\n",
    "            # keep track of the metrices\n",
    "            with summary_writer.as_default():\n",
    "                for metric in model.metrics: \n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the training metrics\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "        # reset metrics \n",
    "        model.reset_metrics()\n",
    "    \n",
    "        # computing the cosine similarity to evaluate if our embedding table is grouping together words with similar semantic meanings\n",
    "        sim = model.validate(val_ds).numpy()\n",
    "        # evaluate the training by printing the closest words to our validation words\n",
    "        for i, word in enumerate(val_words):\n",
    "            top_k = 8 # number of nearest neighbors\n",
    "            log = f\"Nearest to {word}: \"\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "            for k in range(top_k):\n",
    "                neighbour = inverse_vocab[nearest[k]]\n",
    "                log += f\" {neighbour},\"\n",
    "            print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pe4-NMJVGJYj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:33<00:00, 106.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 31.33449363708496']\n",
      "Nearest to holy:  righteousness, sent, god, so, now, light, whom, hath,\n",
      "Nearest to father:  king, house, land, saying, day, lord, give, great,\n",
      "Nearest to wine:  fled, shut, prepare, open, born, established, worshipped, turn,\n",
      "Nearest to poison:  hundreds, devoured, confusion, suffice, frankincense, fishes, remove, sanctified,\n",
      "Nearest to love:  would, see, should, cast, who, own, moreover, death,\n",
      "Nearest to strong:  we, people, lord, let, stand, days, cast, because,\n",
      "Nearest to day:  lord, what, as, but, chosen, death, yet, flesh,\n",
      "Epoch 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:31<00:00, 108.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 8.095026016235352']\n",
      "Nearest to holy:  righteousness, light, knoweth, crying, strengthen, giveth, obey, sign,\n",
      "Nearest to father:  servant, wife, sake, chosen, king, house, throne, glory,\n",
      "Nearest to wine:  fled, prepare, shut, confess, declare, write, standing, hid,\n",
      "Nearest to poison:  hundreds, confusion, suffice, fishes, frankincense, devoured, remove, sanctified,\n",
      "Nearest to love:  moreover, would, arise, see, lift, whether, fountain, sign,\n",
      "Nearest to strong:  washed, blind, ground, sleep, live, seek, salvation, arise,\n",
      "Nearest to day:  weeping, raised, killed, lord, shield, what, trespass, abode,\n",
      "Epoch 2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:31<00:00, 108.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.496453285217285']\n",
      "Nearest to holy:  strengthen, remaineth, served, giveth, knoweth, light, righteousness, crying,\n",
      "Nearest to father:  servant, sake, wife, oath, jonathan, glory, chosen, presence,\n",
      "Nearest to wine:  confess, fled, prepare, business, falleth, write, shut, sickle,\n",
      "Nearest to poison:  hundreds, suffice, confusion, fishes, devoured, remove, determined, repay,\n",
      "Nearest to love:  seek, arise, ways, sign, lift, vineyards, ought, moreover,\n",
      "Nearest to strong:  washed, strengthen, trust, blind, sleep, salvation, believe, seek,\n",
      "Nearest to day:  balaam, raised, killed, time, beginning, weeping, light, abode,\n",
      "Epoch 3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:30<00:00, 109.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.070240497589111']\n",
      "Nearest to holy:  served, strengthen, remaineth, giveth, beautiful, sign, knoweth, gentiles,\n",
      "Nearest to father:  servant, sake, jonathan, presence, oath, womb, husband, shewed,\n",
      "Nearest to wine:  confess, falleth, business, sickle, prepare, washed, continue, shut,\n",
      "Nearest to poison:  hundreds, suffice, confusion, devoured, determined, scribe, remove, enchantments,\n",
      "Nearest to love:  seek, ways, teach, understand, sign, forgive, ought, lamentation,\n",
      "Nearest to strong:  washed, blind, trust, believe, strengthen, food, seek, sleep,\n",
      "Nearest to day:  balaam, beginning, trouble, light, time, hearkened, month, killed,\n",
      "Epoch 4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:30<00:00, 109.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.046149253845215']\n",
      "Nearest to holy:  beautiful, vale, giveth, where, least, sign, remaineth, served,\n",
      "Nearest to father:  servant, sake, jonathan, husband, presence, youth, womb, shewed,\n",
      "Nearest to wine:  falleth, business, confess, sickle, continue, copy, merry, food,\n",
      "Nearest to poison:  hundreds, suffice, determined, devoured, confusion, remove, cometh, enchantments,\n",
      "Nearest to love:  seek, lamentation, teach, understand, rejoice, desire, ways, ought,\n",
      "Nearest to strong:  believe, washed, trust, strengthen, food, confounded, upright, blind,\n",
      "Nearest to day:  balaam, beginning, hour, month, light, time, dwellings, trouble,\n",
      "Epoch 5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:29<00:00, 109.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 5.973578453063965']\n",
      "Nearest to holy:  lamentation, beautiful, vale, touching, love, dealeth, high, remaineth,\n",
      "Nearest to father:  servant, sake, jonathan, youth, camps, husband, womb, shewed,\n",
      "Nearest to wine:  merry, oil, continue, business, falleth, sickle, copy, wheat,\n",
      "Nearest to poison:  hundreds, devoured, odours, suffice, determined, enchantments, dungeon, acceptable,\n",
      "Nearest to love:  seek, desire, rejoice, riding, lamentation, teach, understand, oblation,\n",
      "Nearest to strong:  believe, confounded, trust, food, became, bestowed, washed, upright,\n",
      "Nearest to day:  hour, dwellings, month, even, beginning, balaam, time, yet,\n",
      "Epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:29<00:00, 109.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.006770610809326']\n",
      "Nearest to holy:  love, high, luz, dealeth, lamentation, beautiful, vale, where,\n",
      "Nearest to father:  servant, jonathan, husband, youth, sake, camps, womb, promise,\n",
      "Nearest to wine:  oil, merry, business, continue, wheat, falleth, copy, sickle,\n",
      "Nearest to poison:  odours, devoured, dungeon, determined, oven, suffice, lean, hundreds,\n",
      "Nearest to love:  seek, riding, rejoice, understand, desire, saidst, teach, prosper,\n",
      "Nearest to strong:  became, confounded, trust, believe, food, upright, flaming, purge,\n",
      "Nearest to day:  hour, month, even, time, dwellings, sabbath, yet, word,\n",
      "Epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:29<00:00, 110.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.036264419555664']\n",
      "Nearest to holy:  luz, love, high, lamentation, dealeth, beautiful, touching, where,\n",
      "Nearest to father:  servant, sake, jonathan, husband, youth, womb, performed, camps,\n",
      "Nearest to wine:  oil, merry, vinegar, business, continue, fail, falleth, privately,\n",
      "Nearest to poison:  odours, dungeon, devoured, lean, determined, oven, wells, possible,\n",
      "Nearest to love:  seek, riding, desire, sakes, understand, rejoice, skill, prosper,\n",
      "Nearest to strong:  became, flaming, confounded, believe, persuade, trust, frame, jew,\n",
      "Nearest to day:  hour, even, month, yet, sabbath, pass, word, dwellings,\n",
      "Epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:28<00:00, 110.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.0014777183532715']\n",
      "Nearest to holy:  high, love, luz, dealeth, lamentation, where, accord, true,\n",
      "Nearest to father:  servant, sake, jonathan, master, husband, mother, performed, promise,\n",
      "Nearest to wine:  oil, merry, vinegar, else, privately, fail, continue, answer,\n",
      "Nearest to poison:  dungeon, odours, devoured, lean, oven, determined, possible, sober,\n",
      "Nearest to love:  rejoice, seek, riding, believe, sakes, understand, desire, saidst,\n",
      "Nearest to strong:  became, flaming, believe, frame, fool, jew, confounded, trust,\n",
      "Nearest to day:  hour, even, month, yet, wherefore, commandment, finished, sabbath,\n",
      "Epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:28<00:00, 110.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.091377258300781']\n",
      "Nearest to holy:  high, love, luz, dealeth, where, accord, touching, affrighted,\n",
      "Nearest to father:  servant, master, jonathan, sake, husband, mother, damnation, promise,\n",
      "Nearest to wine:  oil, merry, vinegar, fail, else, continue, bottles, corn,\n",
      "Nearest to poison:  dungeon, devoured, odours, possible, pressed, oven, sober, repay,\n",
      "Nearest to love:  rejoice, seek, believe, riding, sakes, desire, saidst, understand,\n",
      "Nearest to strong:  became, flaming, believe, fool, frame, exalted, trust, confounded,\n",
      "Nearest to day:  even, hour, month, commandment, pray, yet, wherefore, when,\n",
      "Epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:27<00:00, 111.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.153596878051758']\n",
      "Nearest to holy:  high, love, luz, accord, where, lamentation, touching, dealeth,\n",
      "Nearest to father:  servant, master, sake, health, jonathan, mother, husband, damnation,\n",
      "Nearest to wine:  oil, merry, vinegar, answer, fail, else, yet, bottles,\n",
      "Nearest to poison:  dungeon, devoured, pressed, oven, sober, possible, spoilers, hedge,\n",
      "Nearest to love:  seek, desire, believe, perfect, rejoice, sakes, prosper, understand,\n",
      "Nearest to strong:  became, fool, flaming, exalted, believe, cruel, trust, foolish,\n",
      "Nearest to day:  even, hour, commandment, month, when, yet, wherefore, last,\n",
      "Epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:27<00:00, 111.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.139459609985352']\n",
      "Nearest to holy:  high, love, accord, where, luz, persecutors, touching, oblation,\n",
      "Nearest to father:  servant, health, master, mother, damnation, jonathan, husband, sake,\n",
      "Nearest to wine:  oil, merry, answer, else, vinegar, yet, flesh, poured,\n",
      "Nearest to poison:  dungeon, devoured, pressed, sober, dawning, spoilers, wells, possible,\n",
      "Nearest to love:  seek, believe, desire, perfect, rejoice, prosper, skill, sakes,\n",
      "Nearest to strong:  became, fool, flaming, cruel, ready, exalted, foolish, trust,\n",
      "Nearest to day:  hour, even, commandment, yet, last, when, wherefore, month,\n",
      "Epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:27<00:00, 111.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.2075276374816895']\n",
      "Nearest to holy:  high, accord, luz, love, where, oblation, doctrines, persecutors,\n",
      "Nearest to father:  servant, health, master, jonathan, damnation, performed, changed, promise,\n",
      "Nearest to wine:  oil, poured, vinegar, merry, answer, flesh, corn, bottles,\n",
      "Nearest to poison:  pressed, dungeon, sober, spoilers, dawning, gileadite, devoured, reserved,\n",
      "Nearest to love:  rejoice, seek, desire, sakes, skill, believe, understand, prosper,\n",
      "Nearest to strong:  became, flaming, fool, cruel, ready, exalted, foolish, frame,\n",
      "Nearest to day:  hour, even, last, commandment, month, yet, when, lord,\n",
      "Epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:27<00:00, 111.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.2622528076171875']\n",
      "Nearest to holy:  high, luz, accord, doctrines, love, most, where, oblation,\n",
      "Nearest to father:  servant, health, master, damnation, jonathan, performed, shoe, promise,\n",
      "Nearest to wine:  oil, corn, poured, vinegar, fail, bottles, answer, merry,\n",
      "Nearest to poison:  pressed, dungeon, dawning, gileadite, reserved, spoilers, sober, devoured,\n",
      "Nearest to love:  seek, comforter, rejoice, believe, desire, skill, prosper, sakes,\n",
      "Nearest to strong:  fool, cruel, became, exalted, ready, flaming, power, foolish,\n",
      "Nearest to day:  hour, even, last, commandment, month, coming, sabbath, fourteenth,\n",
      "Epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16442/16442 [02:27<00:00, 111.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 6.316689968109131']\n",
      "Nearest to holy:  luz, accord, love, doctrines, high, most, where, oblation,\n",
      "Nearest to father:  servant, master, performed, shoe, health, damnation, bonds, changed,\n",
      "Nearest to wine:  oil, breath, corn, merry, nails, vinegar, eaten, bottles,\n",
      "Nearest to poison:  pressed, dawning, dungeon, reserved, gileadite, cieled, spoilers, sober,\n",
      "Nearest to love:  comforter, rejoice, skill, sakes, desire, perfect, saidst, believe,\n",
      "Nearest to strong:  cruel, fool, became, ready, exalted, flaming, foolish, power,\n",
      "Nearest to day:  hour, even, last, fourteenth, commandment, coming, sabbath, month,\n"
     ]
    }
   ],
   "source": [
    "# Keep track of the counts\n",
    "_, _, counts = map(lambda x:x.numpy(), tf.unique_with_counts(txt))\n",
    "counts = list(counts.astype(float)/len(txt))\n",
    "\n",
    "# create a tf data set with the indices of our validation words\n",
    "val_words = [\"holy\", \"father\", \"wine\", \"poison\", \"love\", \"strong\", \"day\"]\n",
    "val_words_idx = [vocab[val_words[i]] for i in range(len(val_words))]\n",
    "val_ds = tf.constant(val_words_idx, dtype=tf.int32)\n",
    "\n",
    "vocabulary_size = len(vocab)\n",
    "embedding_size = 64\n",
    "epochs = 15\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.001) \n",
    "summary_writer = create_summary_writer(config_name = f'RUN')\n",
    "\n",
    "model = SkipGramModel(optimizer = optimizer,\n",
    "                      embedding_size = embedding_size,\n",
    "                      vocabulary_size = vocabulary_size,\n",
    "                      counts = counts)\n",
    "\n",
    "training_loop(ds, model, epochs, val_ds, val_words, summary_writer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
