{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7HSdnSt3ejLn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import datetime\n",
        "import tqdm\n",
        "import re\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.1 The dataset**"
      ],
      "metadata": {
        "id": "3wJHEaVhmU4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bash code to mount the drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"drive/MyDrive\")"
      ],
      "metadata": {
        "id": "D2AwYgxJerM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b971a849-8c9c-4f76-cde9-741629026bfb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 the dataset\n",
        "with open(\"/content/bible.txt\", \"r\") as f:\n",
        "  txt = f.read()\n",
        "\n",
        "# print a short example\n",
        "print(txt[:500])"
      ],
      "metadata": {
        "id": "heUMdMV3fY6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb17537a-19c0-450b-858b-16751b3bc580"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth.\n",
            "\n",
            "1:2 And the earth was without form, and void; and darkness was upon\n",
            "the face of the deep. And the Spirit of God moved upon the face of the\n",
            "waters.\n",
            "\n",
            "1:3 And God said, Let there be light: and there was light.\n",
            "\n",
            "1:4 And God saw the light, that it was good: and God divided the light\n",
            "from the darkness.\n",
            "\n",
            "1:5 And God called the light Day, and the darkness he called Night.\n",
            "And the evening and the mornin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.2 Word embeddings**"
      ],
      "metadata": {
        "id": "CU_rjW6Rm9Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to lower case, remove \\n and special characters\n",
        "txt = txt.replace(\"\\n\", \" \").lower()\n",
        "txt = re.sub('[^A-Za-z]+', ' ', txt) \n",
        "print(f\"converted: {txt[:100]} \\n\")\n",
        "\n",
        "# tokenize the text\n",
        "txt = tf_text.WhitespaceTokenizer().split(txt)\n",
        "txt = list(txt.numpy().astype('U'))\n",
        "print(f\"tokenized: {txt[:50]}\")"
      ],
      "metadata": {
        "id": "LcU151XIRA-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9247b241-8aea-4f1c-dc32-d9a4ae5d3bd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converted: the first book of moses called genesis in the beginning god created the heaven and the earth and the \n",
            "\n",
            "tokenized: ['the', 'first', 'book', 'of', 'moses', 'called', 'genesis', 'in', 'the', 'beginning', 'god', 'created', 'the', 'heaven', 'and', 'the', 'earth', 'and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness', 'was', 'upon', 'the', 'face', 'of', 'the', 'deep', 'and', 'the', 'spirit', 'of', 'god', 'moved', 'upon', 'the', 'face', 'of', 'the', 'waters', 'and', 'god', 'said', 'let']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# counting all words from the corpus and get 10000 most frequent words\n",
        "count = Counter(txt).most_common(10000)\n",
        "words = [x[0] for x in count]\n",
        "print(\"The 20 most common words: \", words[:50])"
      ],
      "metadata": {
        "id": "WNXubl-x8oA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a3a390-893a-42cc-e637-20f8635e3397"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 20 most common words:  ['the', 'and', 'of', 'to', 'that', 'in', 'he', 'shall', 'unto', 'for', 'i', 'his', 'a', 'lord', 'they', 'be', 'is', 'him', 'not', 'them', 'it', 'with', 'all', 'thou', 'thy', 'was', 'god', 'which', 'my', 'me', 'said', 'but', 'ye', 'their', 'have', 'will', 'thee', 'from', 'as', 'are', 'when', 'this', 'out', 'were', 'upon', 'man', 'by', 'you', 'israel', 'king']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary to save mappings from tokens to integer indices\n",
        "vocab = {token: idx for idx, token in enumerate(words)}\n",
        "# create another dictionary to store the inverse mapping\n",
        "inverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# convert the words into integers\n",
        "txt = [vocab[word] for word in txt if word in vocab.keys()]\n",
        "print(txt[:50])\n",
        "print(len(txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-E-8Rjpnmgo",
        "outputId": "9a8c4790-6fc7-4ad1-874f-4d3c1b3ff56c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 216, 401, 2, 132, 160, 8616, 5, 0, 679, 26, 1295, 0, 170, 1, 0, 111, 1, 0, 111, 25, 220, 1999, 1, 2000, 1, 497, 25, 44, 0, 227, 2, 0, 971, 1, 0, 189, 2, 26, 867, 44, 0, 227, 2, 0, 304, 1, 26, 30, 79]\n",
            "789262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0.001\n",
        "word_counts = Counter(txt)\n",
        "total_count = len(txt)\n",
        "\n",
        "# calculating for each word the fraction of the total words being this specific word\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "# calculating for each word the probability of keeping this specific word\n",
        "p_keep = {word: ((np.sqrt(freqs[word]/s) + 1)*s/freqs[word]) for word in word_counts}\n",
        "\n",
        "# apply subsampling to discard words that appear very often\n",
        "txt = [word for word in txt if random.random() < p_keep[word]]\n",
        "print(txt[:50])\n",
        "print(len(txt))"
      ],
      "metadata": {
        "id": "F0zH27Q48oKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb37f6b3-a215-4d01-a88b-6c06ff508b53"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[216, 401, 2, 132, 160, 8616, 679, 1295, 170, 111, 111, 220, 1999, 2000, 1, 497, 44, 227, 971, 189, 26, 867, 227, 304, 79, 52, 15, 326, 52, 25, 326, 177, 326, 4, 25, 147, 26, 930, 0, 326, 497, 26, 160, 326, 70, 497, 160, 286, 1039, 387]\n",
            "526231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the input-target pairs\n",
        "pairs = []\n",
        "for i in range(len(txt)):\n",
        "    for j in [-2, -1, 1, 2]: # window size of 4\n",
        "        if i + j >= 0 and i + j < len(txt):\n",
        "            pairs.append((txt[i], txt[i + j]))\n",
        "\n",
        "for pair in pairs[:15]:\n",
        "    print(f\"input word: {inverse_vocab[pair[0]]}, target word: {inverse_vocab[pair[1]]} \\n\")"
      ],
      "metadata": {
        "id": "elimbCvGRl_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32c8867-c8cb-4a25-f878-67971a6f8e46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: first, target word: book \n",
            "\n",
            "input word: first, target word: of \n",
            "\n",
            "input word: book, target word: first \n",
            "\n",
            "input word: book, target word: of \n",
            "\n",
            "input word: book, target word: moses \n",
            "\n",
            "input word: of, target word: first \n",
            "\n",
            "input word: of, target word: book \n",
            "\n",
            "input word: of, target word: moses \n",
            "\n",
            "input word: of, target word: called \n",
            "\n",
            "input word: moses, target word: book \n",
            "\n",
            "input word: moses, target word: of \n",
            "\n",
            "input word: moses, target word: called \n",
            "\n",
            "input word: moses, target word: genesis \n",
            "\n",
            "input word: called, target word: of \n",
            "\n",
            "input word: called, target word: moses \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the data set\n",
        "pairs = np.array(pairs)\n",
        "ds = tf.data.Dataset.from_tensor_slices((pairs[:,0], pairs[:,1]))\n",
        "\n",
        "# shuffle, batch, prefetch and cache\n",
        "ds = ds.shuffle(1024).batch(128, drop_remainder=True).prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "# investigate the newly created data set\n",
        "for batch in ds.take(1):\n",
        "    tf.print(tf.shape(batch), \"\\n\", batch)"
      ],
      "metadata": {
        "id": "8K_jBBoORmIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2156424-3dfc-4d95-b78f-41b81ecad5c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 128] \n",
            " ([160 3 111 ... 387 1167 1216], [71 956 170 ... 170 6 37])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.3 Model**"
      ],
      "metadata": {
        "id": "tZ6JPRs_nF9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, optimizer, embedding_size, vocabulary_size, counts, num_negative_samples=64):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.optimizer = optimizer\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.counts = counts\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        \n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\")]\n",
        "        \n",
        "        # Neural network weights and biases\n",
        "        self.nce_weights = tf.Variable(tf.random.truncated_normal([self.vocabulary_size, self.embedding_size], stddev=0.1 / np.sqrt(self.embedding_size)))\n",
        "        self.nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = tf.Variable(tf.random.uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
        "\n",
        "    def call(self, input_words):\n",
        "        # Look up embeddings for a batch of inputs\n",
        "        embed = tf.nn.embedding_lookup(self.embedding, input_words)\n",
        "    \n",
        "        return embed\n",
        "    \n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "\n",
        "    def get_nce_loss(self, target_words, input_words, embed):\n",
        "        # Sample a number of negative samples for given input words\n",
        "        sampled_values = tf.random.fixed_unigram_candidate_sampler(true_classes=tf.reshape(input_words, (128,1)),\n",
        "                                                                   num_true=1,\n",
        "                                                                   num_sampled=self.num_negative_samples,\n",
        "                                                                   unique=True,\n",
        "                                                                   range_max=self.vocabulary_size,\n",
        "                                                                   unigrams=self.counts,\n",
        "                                                                   name=\"negative_sampling\")\n",
        "        \n",
        "        # Compute the noise contrastive loss\n",
        "        nce_loss = tf.nn.nce_loss(weights =self.nce_weights,\n",
        "                                  biases=self.nce_biases,\n",
        "                                  labels=target_words, \n",
        "                                  inputs=embed,  \n",
        "                                  num_sampled=self.num_negative_samples, \n",
        "                                  num_classes=self.vocabulary_size,\n",
        "                                  sampled_values=sampled_values)\n",
        "        \n",
        "        return tf.reduce_mean(nce_loss)\n",
        "    \n",
        "    @tf.function\n",
        "    def train(self, data):\n",
        "        input_words, target_words = data\n",
        "      \n",
        "        with tf.GradientTape() as tape:\n",
        "            # get the embedding\n",
        "            embed = self(input_words)\n",
        "            # compute the loss\n",
        "            loss = self.get_nce_loss(target_words, input_words, embed)\n",
        "        \n",
        "        # compute the gradients and update the network\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update the metrics \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "    @tf.function\n",
        "    def validate(self, val_ds):\n",
        "        # Compute the cosine similarity between a few common words and all embeddings\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(self.embedding), 1, keepdims=True))\n",
        "        normalized_embedding = self.embedding / norm\n",
        "        val_embedding = tf.nn.embedding_lookup(normalized_embedding, val_ds)\n",
        "        sim = tf.matmul(val_embedding, tf.transpose(normalized_embedding))\n",
        "        \n",
        "        return sim"
      ],
      "metadata": {
        "id": "AmNoNm0sG-Ro"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.4 Training**"
      ],
      "metadata": {
        "id": "uOLztu7jnKsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_summary_writer(config_name):\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    log_path = f\"logs/skipgram/{config_name}/{current_time}\"\n",
        "    summary_writer = tf.summary.create_file_writer(log_path)\n",
        "\n",
        "    return summary_writer"
      ],
      "metadata": {
        "id": "DEGUCTSUGlV3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(ds, model, epochs, val_ds, val_words, summary_writer): \n",
        "    # iterate over given amount of epochs\n",
        "    for epoch in range(epochs):     \n",
        "        print(f\"Epoch {epoch}: \")\n",
        "\n",
        "        for data in tqdm.tqdm(ds, position = 0, leave = True):\n",
        "            # get the loss\n",
        "            metrics = model.train(data)\n",
        "\n",
        "            # keep track of the metrices\n",
        "            with summary_writer.as_default():\n",
        "                for metric in model.metrics: \n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the training metrics\n",
        "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "        # reset metrics \n",
        "        model.reset_metrics()\n",
        "    \n",
        "        # computing the cosine similarity to evaluate if our embedding table is grouping together words with similar semantic meanings\n",
        "        sim = model.validate(val_ds).numpy()\n",
        "        # evaluate the training by printing the closest words to our validation words\n",
        "        for i, word in enumerate(val_words):\n",
        "            top_k = 8 # number of nearest neighbors\n",
        "            log = f\"Nearest to {word}: \"\n",
        "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
        "            for k in range(top_k):\n",
        "                neighbour = inverse_vocab[nearest[k]]\n",
        "                log += f\" {neighbour},\"\n",
        "            print(log)"
      ],
      "metadata": {
        "id": "pe4-NMJVGJYj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep track of the counts\n",
        "_, _, counts = map(lambda x:x.numpy(), tf.unique_with_counts(txt))\n",
        "counts = list(counts.astype(float)/len(txt))\n",
        "\n",
        "# create a tf data set with the indices of our validation words\n",
        "val_words = [\"holy\", \"father\", \"wine\", \"poison\", \"love\", \"strong\", \"day\"]\n",
        "val_words_idx = [vocab[val_words[i]] for i in range(len(val_words))]\n",
        "val_ds = tf.constant(val_words_idx, dtype=tf.int32)\n",
        "\n",
        "vocabulary_size = len(vocab)\n",
        "embedding_size = 64\n",
        "epochs = 15\n",
        "\n",
        "optimizer = tf.optimizers.Adam(0.001) \n",
        "summary_writer = create_summary_writer(config_name = f'RUN')\n",
        "\n",
        "model = SkipGramModel(optimizer = optimizer,\n",
        "                      embedding_size = embedding_size,\n",
        "                      vocabulary_size = vocabulary_size,\n",
        "                      counts = counts)\n",
        "\n",
        "training_loop(ds, model, epochs, val_ds, val_words, summary_writer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mg1rTDKcwQb",
        "outputId": "50eb30e4-2c66-4e5a-dd15-a7166adb4d86"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [07:21<00:00, 37.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 31.258331298828125']\n",
            "Nearest to holy:  before, deliver, stood, children, brother, peace, again, now,\n",
            "Nearest to father:  fathers, men, mother, servant, this, say, said, us,\n",
            "Nearest to wine:  they, bring, do, people, turn, away, seen, thee,\n",
            "Nearest to poison:  branches, better, slew, magnified, man, brute, searched, truth,\n",
            "Nearest to love:  life, which, done, seek, by, would, built, ready,\n",
            "Nearest to strong:  teach, cause, time, is, ground, yea, cast, did,\n",
            "Nearest to day:  know, say, do, hearken, sea, prophets, many, spirit,\n",
            "Epoch 1: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 8.107242584228516']\n",
            "Nearest to holy:  else, worshipped, manner, joined, almighty, iniquities, habitation, whole,\n",
            "Nearest to father:  mother, servant, solomon, confess, fathers, knowest, ears, flock,\n",
            "Nearest to wine:  read, receive, sure, live, stronger, they, hired, feared,\n",
            "Nearest to poison:  better, branches, searched, slew, magnified, truth, turneth, beat,\n",
            "Nearest to love:  life, please, fallen, bow, remain, hated, truly, which,\n",
            "Nearest to strong:  teach, thereon, ground, vain, cause, whereof, child, yea,\n",
            "Nearest to day:  hill, loosed, hold, deeds, prophets, goeth, vain, understood,\n",
            "Epoch 2: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.477917671203613']\n",
            "Nearest to holy:  joined, almighty, else, figs, manner, iniquities, habitation, service,\n",
            "Nearest to father:  mother, servant, confess, solomon, jezreel, oath, follow, add,\n",
            "Nearest to wine:  receive, read, stronger, hired, learn, behalf, churches, lives,\n",
            "Nearest to poison:  better, searched, truth, slew, chasten, branches, turneth, magnified,\n",
            "Nearest to love:  life, please, bow, fallen, remain, declare, remember, truly,\n",
            "Nearest to strong:  teach, thereon, price, vain, bitter, root, pleasant, continue,\n",
            "Nearest to day:  hill, first, loosed, seventh, fourteenth, whilst, understood, passover,\n",
            "Epoch 3: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:27<00:00, 50.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.055075168609619']\n",
            "Nearest to holy:  almighty, joined, habitation, figs, else, created, evermore, building,\n",
            "Nearest to father:  mother, servant, confess, abraham, oath, wealth, jezreel, solomon,\n",
            "Nearest to wine:  receive, eat, drink, behalf, hired, stronger, considered, learn,\n",
            "Nearest to poison:  chasten, truth, better, searched, abel, turneth, please, slew,\n",
            "Nearest to love:  life, declare, bow, perish, please, remember, judge, gifts,\n",
            "Nearest to strong:  price, teach, root, bitter, continue, remembrance, soon, valleys,\n",
            "Nearest to day:  fourteenth, seventh, first, passover, hill, night, understood, vision,\n",
            "Epoch 4: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 5.942498207092285']\n",
            "Nearest to holy:  almighty, habitation, service, created, joined, figs, hosts, evermore,\n",
            "Nearest to father:  oath, salvation, abraham, servant, confess, wealth, flock, tongue,\n",
            "Nearest to wine:  drink, eat, receive, satisfied, rage, drank, holds, behalf,\n",
            "Nearest to poison:  chasten, abel, better, truth, slew, please, need, searched,\n",
            "Nearest to love:  life, declare, judge, bow, perish, would, slain, killed,\n",
            "Nearest to strong:  root, price, bitter, remembrance, continue, pleasant, teach, soon,\n",
            "Nearest to day:  fourteenth, seventh, passover, first, night, vision, understood, hosts,\n",
            "Epoch 5: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 5.925527095794678']\n",
            "Nearest to holy:  almighty, redeemer, hosts, seemed, service, behold, house, subjection,\n",
            "Nearest to father:  abraham, oath, salvation, confess, wealth, servant, rebellion, saul,\n",
            "Nearest to wine:  drink, satisfied, eat, rage, holds, drank, behalf, strong,\n",
            "Nearest to poison:  chasten, abel, better, slew, vessel, truth, refuge, makest,\n",
            "Nearest to love:  life, declare, slain, perish, killed, why, please, wherefore,\n",
            "Nearest to strong:  root, price, bitter, minds, remembrance, soon, scourge, misery,\n",
            "Nearest to day:  passover, seventh, fourteenth, first, vision, night, hosts, morrow,\n",
            "Epoch 6: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:28<00:00, 50.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 5.970157146453857']\n",
            "Nearest to holy:  almighty, redeemer, behold, hosts, within, house, seemed, service,\n",
            "Nearest to father:  abraham, oath, saul, salvation, beor, confess, wealth, jesus,\n",
            "Nearest to wine:  drink, satisfied, rage, eat, holds, drive, drank, cup,\n",
            "Nearest to poison:  abel, vessel, makest, following, chasten, refuge, please, anguish,\n",
            "Nearest to love:  declare, life, slain, killed, why, seemeth, remember, wouldest,\n",
            "Nearest to strong:  root, price, minds, bitter, scourge, misery, consent, mark,\n",
            "Nearest to day:  passover, seventh, fourteenth, first, vision, night, cleansing, convenient,\n",
            "Epoch 7: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.106332778930664']\n",
            "Nearest to holy:  behold, within, hosts, redeemer, seemed, house, almighty, service,\n",
            "Nearest to father:  saul, abraham, oath, salvation, jesus, servant, beor, gift,\n",
            "Nearest to wine:  drink, drive, rage, satisfied, eat, holds, cup, drank,\n",
            "Nearest to poison:  vessel, abel, makest, oftentimes, anguish, chasten, following, ministers,\n",
            "Nearest to love:  declare, choose, liveth, wouldest, seemeth, wrong, why, killed,\n",
            "Nearest to strong:  root, minds, price, bitter, scourge, misery, consent, seeing,\n",
            "Nearest to day:  seventh, passover, first, night, fourteenth, vision, likewise, convenient,\n",
            "Epoch 8: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:21<00:00, 51.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.105348587036133']\n",
            "Nearest to holy:  behold, within, hosts, seemed, house, zion, redeemer, adulteries,\n",
            "Nearest to father:  saul, abraham, oath, jesus, salvation, servant, who, gift,\n",
            "Nearest to wine:  drink, rage, drive, satisfied, eat, holds, morsel, fellows,\n",
            "Nearest to poison:  abel, vessel, makest, oftentimes, ministers, repenteth, anguish, offence,\n",
            "Nearest to love:  declare, choose, wouldest, why, killed, determined, likewise, slain,\n",
            "Nearest to strong:  root, minds, bitter, price, scourge, seeing, soon, mark,\n",
            "Nearest to day:  seventh, first, passover, convenient, likewise, fourteenth, night, even,\n",
            "Epoch 9: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:21<00:00, 51.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.12803840637207']\n",
            "Nearest to holy:  hosts, within, behold, seemed, love, adulteries, called, zion,\n",
            "Nearest to father:  saul, abraham, oath, servants, jesus, faith, who, salvation,\n",
            "Nearest to wine:  drink, drive, eat, rage, satisfied, morsel, holds, fellows,\n",
            "Nearest to poison:  vessel, oftentimes, abel, makest, repenteth, offence, theirs, skill,\n",
            "Nearest to love:  declare, choose, widows, wouldest, mayest, likewise, truth, slain,\n",
            "Nearest to strong:  root, bitter, minds, scourge, soon, seeing, price, consent,\n",
            "Nearest to day:  seventh, first, even, likewise, now, night, passover, convenient,\n",
            "Epoch 10: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.234816551208496']\n",
            "Nearest to holy:  hosts, seemed, behold, love, truth, within, called, zion,\n",
            "Nearest to father:  saul, abraham, oath, faith, master, servants, liar, jesus,\n",
            "Nearest to wine:  drink, satisfied, eat, drive, rage, hindered, morsel, fellows,\n",
            "Nearest to poison:  oftentimes, vessel, abel, makest, filthiness, threw, offence, repenteth,\n",
            "Nearest to love:  choose, declare, loved, widows, truth, determined, wouldest, swallowed,\n",
            "Nearest to strong:  root, bitter, minds, soon, seeing, price, consent, scourge,\n",
            "Nearest to day:  seventh, first, now, even, convenient, likewise, passover, hosts,\n",
            "Epoch 11: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:25<00:00, 50.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.25201940536499']\n",
            "Nearest to holy:  hosts, behold, zion, seemed, within, love, called, truth,\n",
            "Nearest to father:  abraham, saul, faith, oath, god, master, wealth, jesus,\n",
            "Nearest to wine:  drink, eat, satisfied, hindered, drive, rage, morsel, fellows,\n",
            "Nearest to poison:  vessel, filthiness, oftentimes, threw, makest, abel, offence, jahaziel,\n",
            "Nearest to love:  choose, loved, widows, stay, declare, now, wrong, determined,\n",
            "Nearest to strong:  root, bitter, seeing, minds, soon, price, water, consent,\n",
            "Nearest to day:  seventh, first, convenient, now, even, hosts, likewise, fourteenth,\n",
            "Epoch 12: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [06:21<00:00, 43.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.317963600158691']\n",
            "Nearest to holy:  hosts, behold, seemed, zion, called, house, say, truth,\n",
            "Nearest to father:  saul, abraham, master, faith, beor, kindred, liar, wealth,\n",
            "Nearest to wine:  drink, eat, satisfied, hindered, drive, holds, increase, vinegar,\n",
            "Nearest to poison:  vessel, threw, filthiness, oftentimes, jahaziel, ministers, offence, skill,\n",
            "Nearest to love:  loved, choose, widows, stay, wrong, swallowed, declare, now,\n",
            "Nearest to strong:  root, bitter, seeing, water, consent, soon, scourge, whether,\n",
            "Nearest to day:  seventh, first, fourteenth, morrow, convenient, hour, hosts, sabbath,\n",
            "Epoch 13: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:20<00:00, 51.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.318968772888184']\n",
            "Nearest to holy:  hosts, behold, seemed, redeemer, called, desired, house, justified,\n",
            "Nearest to father:  saul, jesus, abraham, kindred, oath, faith, wealth, who,\n",
            "Nearest to wine:  drink, eat, satisfied, vinegar, cup, hindered, drive, increase,\n",
            "Nearest to poison:  vessel, threw, filthiness, oftentimes, jahaziel, offence, firmament, natural,\n",
            "Nearest to love:  widows, wrong, loved, swallowed, choose, now, stay, endured,\n",
            "Nearest to strong:  root, bitter, seeing, water, gathering, consent, misery, price,\n",
            "Nearest to day:  seventh, morrow, fourteenth, first, convenient, sabbath, passover, hour,\n",
            "Epoch 14: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16444/16444 [05:21<00:00, 51.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss: 6.378677845001221']\n",
            "Nearest to holy:  hosts, behold, called, seemed, redeemer, truth, healing, desired,\n",
            "Nearest to father:  wealth, abraham, god, kindred, jesus, saul, dance, who,\n",
            "Nearest to wine:  drink, eat, vinegar, satisfied, spirit, drive, loft, cup,\n",
            "Nearest to poison:  vessel, threw, filthiness, oftentimes, jahaziel, calleth, ministers, offence,\n",
            "Nearest to love:  wrong, choose, loved, widows, swallowed, stay, creep, now,\n",
            "Nearest to strong:  seeing, root, gathering, water, famous, bitter, trench, deep,\n",
            "Nearest to day:  seventh, morrow, fourteenth, sabbath, hour, passover, until, spring,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}