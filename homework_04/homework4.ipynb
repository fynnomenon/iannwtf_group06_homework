{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "3eQD_OhqmeeB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "\n",
        "#load the mnist dataset\n",
        "(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import datetime\n",
        "\n",
        "# in a notebook, load the tensorboard extension, not needed for scripts\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "WbbSxfE2peOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee957137-0a26-40c8-f6b9-32f32f028660"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_mnist_data(task,batch_size,mnist):\n",
        "  \n",
        "  #flatten the images into one dimensional vector\n",
        "  mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
        "  #convert data from uint8 to float32\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #normalization to have input of range [-1,1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "  #create one-hot vector targets\n",
        "  #mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "  mnist = tf.data.Dataset.zip((mnist.shuffle(2000), mnist.shuffle(2000)))\n",
        "  #mnist = mnist.map(lambda zip(pic1,pic2): (pic1[0],pic2[0],pic1[1],pic2[1]))\n",
        "  mnist = calc(task,mnist)\n",
        "  #mnist = mnist.map(lambda img, target: (img, target ))\n",
        "  #cache this progress in memory - improve performance\n",
        "  mnist = mnist.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  mnist = mnist.shuffle(1000)\n",
        "  mnist = mnist.batch(batch_size)\n",
        "  mnist = mnist.prefetch(20)\n",
        "\n",
        "  #return preprocessed dataset\n",
        "  return mnist\n",
        "\n",
        "#apply the preprocessing to both data sets\n",
        "batch_size = 32\n",
        "train_1 = prepare_mnist_data(1,batch_size,train_ds)\n",
        "test_1 = prepare_mnist_data(1,batch_size,test_ds)\n",
        "\n",
        "train_2 = prepare_mnist_data(2,batch_size,train_ds)\n",
        "test_2 = prepare_mnist_data(2,batch_size,test_ds)"
      ],
      "metadata": {
        "id": "cu6QEg08F9oO"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc(task,mnist):\n",
        "  if task == 1:\n",
        "    mnist = mnist.map(lambda pic1,pic2: (pic1[0],pic2[0],tf.cast((pic1[1] + pic2[1] >= 5), tf.int32)))\n",
        "    return mnist\n",
        "  if task == 2:\n",
        "    mnist = mnist.map(lambda pic1,pic2: (pic1[0],pic2[0], tf.cast((pic1[1]-pic2[1]), tf.int32)))\n",
        "    return mnist\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W-d0ILVuDW9i"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train_2.take(1):\n",
        "  print(\"pic1:\", i[0], \"\\npic2: \", i[1], \"\\ntarget: \",i[2], i[2].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICF7qQwSDbRy",
        "outputId": "5b4c45cc-b8cc-4446-be23-cd5a49820f12"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pic1: tf.Tensor(\n",
            "[[-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " ...\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]], shape=(32, 784), dtype=float32) \n",
            "pic2:  tf.Tensor(\n",
            "[[-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " ...\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]\n",
            " [-1. -1. -1. ... -1. -1. -1.]], shape=(32, 784), dtype=float32) \n",
            "target:  tf.Tensor(\n",
            "[ 7 -6  4  5  5  2  8 -6  3  2  6 -3  3 -3 -6  1  4 -3 -1 -3  5  1 -1 -3\n",
            "  3 -4  5  7  1  1  0 -6], shape=(32,), dtype=int32) (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  The model we are using for predicting the outcome \n",
        "  \"\"\" \n",
        "  def __init__(self, task=1):\n",
        "    \"\"\"\n",
        "    Initializing the model \n",
        "    \"\"\"\n",
        "    #inherit from parent class\n",
        "    super(MyModel, self).__init__()\n",
        "    \n",
        "    if task==1:\n",
        "      self.metrics_list = [tf.keras.metrics.Mean(name = 'loss'),tf.keras.metrics.BinaryAccuracy(name = \"acc\")]\n",
        "      self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "      self.dense1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
        "      self.dense2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
        "      #self.dense3 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
        "      #self.dense1 = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
        "      self.out_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "\n",
        "    elif task==2:\n",
        "      self.metrics_list = [tf.keras.metrics.Mean(name = 'loss'),tf.keras.metrics.MeanSquaredLogarithmicError(name = \"MSLE\")]\n",
        "      self.loss_function = tf.keras.losses.MeanSquaredError()\n",
        "      self.dense1 = tf.keras.layers.Dense(256, activation= tf.nn.relu)\n",
        "      self.dense2 = tf.keras.layers.Dense(256, activation= tf.nn.relu)\n",
        "      # self.dense3 = tf.keras.layers.Dense(256, activation= tf.nn.relu)\n",
        "      # self.dense4 = tf.keras.layers.Dense(256, activation= tf.nn.relu)\n",
        "      self.out_layer = tf.keras.layers.Dense(1, activation= \"linear\")\n",
        "\n",
        "    else: print(\"Exception\")\n",
        "\n",
        "    #self.learning_rate = 0.001\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  @tf.function \n",
        "  def call(self, images):\n",
        "    \"\"\"\n",
        "    how to forward the images through the layer\n",
        "    \"\"\"\n",
        "    img1, img2 = images \n",
        "\n",
        "    img1_x = self.dense1(img1)\n",
        "    img1_x = self.dense2(img1_x)\n",
        "    # img1_x = self.dense3(img1_x)\n",
        "    # img1_x = self.dense4(img1_x)\n",
        "\n",
        "    img2_x = self.dense1(img2)\n",
        "    img2_x = self.dense2(img2_x)\n",
        "    # img2_x = self.dense3(img2_x)\n",
        "    # img2_x = self.dense4(img2_x)\n",
        "\n",
        "    combined = tf.concat([img1_x, img2_x], axis = 1)\n",
        "    out = self.out_layer(combined)\n",
        "\n",
        "    return out\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    img1, img2, t = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self((img1, img2), training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    img1, img2, t = data\n",
        "\n",
        "    output = self((img1,img2), training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "\n",
        "    for metric in self.metrics[1:]:\n",
        "          metric.update_state(t,output)\n",
        "    \n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "dUQ1Kr1dAPXP"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import pprint \n",
        "\n",
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer): \n",
        "  #iterate over given amount of epochs\n",
        "  for epoch in range(epochs): \n",
        "    print(f\"Epoch {epoch}: \")\n",
        "\n",
        "    #train on all batches of the training data\n",
        "    for data in tqdm.tqdm(train_ds, position = 0, leave = True):\n",
        "\n",
        "      metrics = model.train_step(data)\n",
        "\n",
        "      with train_summary_writer.as_default(): \n",
        "        for metric in model.metrics: \n",
        "          tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "    # print the metrics\n",
        "    print([f\"Train {key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    #reset metric \n",
        "    model.reset_metrics()\n",
        "  \n",
        "    #evaluation on validation set\n",
        "    for data in val_ds:\n",
        "      metrics = model.test_step(data)\n",
        "\n",
        "      with val_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "          tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "      \n",
        "    # print the metrics\n",
        "    print([f\"Test {key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    #reset metric\n",
        "    model.reset_metric()\n",
        "\n",
        "    print(\"\\n\")\n",
        "      \n",
        "    "
      ],
      "metadata": {
        "id": "AI2WJpkkMhEO"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(task, train_summary_writer, val_summary_writer):\n",
        "  ### Hyperparameters\n",
        "  num_epochs = 5\n",
        "  batch_size = 32\n",
        "  if task == 1:\n",
        "    train = prepare_mnist_data(1,batch_size,train_ds)\n",
        "    test = prepare_mnist_data(1,batch_size,test_ds)\n",
        "  if task == 2:\n",
        "    train = prepare_mnist_data(2,batch_size,train_ds)\n",
        "    test = prepare_mnist_data(2,batch_size,test_ds)\n",
        "    \n",
        "  train_dataset = train\n",
        "  test_dataset = test\n",
        "\n",
        "  # Initialize the model.\n",
        "  model = MyModel(task)\n",
        "\n",
        "  training_loop(model,train_dataset,test_dataset,num_epochs,train_summary_writer, val_summary_writer)"
      ],
      "metadata": {
        "id": "4qLXtIkAFun0"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_name = \"config_name\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
        "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
        "\n",
        "#log writer \n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "\n",
        "\n",
        "run(2, train_summary_writer, val_summary_writer)"
      ],
      "metadata": {
        "id": "yh-7_Bnt8xKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b409c58d-85d5-4c73-b04d-0f870e2bdf99"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:41<00:00, 45.68it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 2.9105515480041504', 'Train MSLE: 0.1512480080127716']\n",
            "['Test loss: 1.7750355005264282', 'Test MSLE: 0.0992751345038414']\n",
            "\n",
            "\n",
            "Epoch 1: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:16<00:00, 116.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.3685848712921143', 'Train MSLE: 0.07520443201065063']\n",
            "['Test loss: 1.3474905490875244', 'Test MSLE: 0.0782305896282196']\n",
            "\n",
            "\n",
            "Epoch 2: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:20<00:00, 91.61it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.080844521522522', 'Train MSLE: 0.05996792018413544']\n",
            "['Test loss: 1.1865911483764648', 'Test MSLE: 0.065712109208107']\n",
            "\n",
            "\n",
            "Epoch 3: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:16<00:00, 112.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9202974438667297', 'Train MSLE: 0.05052749067544937']\n",
            "['Test loss: 1.045397162437439', 'Test MSLE: 0.05946068465709686']\n",
            "\n",
            "\n",
            "Epoch 4: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1875/1875 [00:20<00:00, 91.62it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.7884718775749207', 'Train MSLE: 0.04384637996554375']\n",
            "['Test loss: 1.0270785093307495', 'Test MSLE: 0.05637187883257866']\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}