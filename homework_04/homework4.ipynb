{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eQD_OhqmeeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5851c586-e98b-46bf-fc4b-354226b1f30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import datetime\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data set\n",
        "mnist = tfds.load('mnist', split = ['train', 'test'], as_supervised=True)\n",
        "\n",
        "#define train and validation set \n",
        "train_ds = mnist[0]\n",
        "val_ds = mnist[1]"
      ],
      "metadata": {
        "id": "WbbSxfE2peOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_mnist_data_problem1(mnist, batch_size):\n",
        "  \"\"\"\n",
        "  Preparation of the data set\n",
        "  Args: \n",
        "    batch_size(int): size of the batches used for the training\n",
        "  Returns: \n",
        "    preprocessed data set\n",
        "  \"\"\"\n",
        "  #convert data from uint8 to float32\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #flatten the images into vectors\n",
        "  mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "  #create tuple with two data points and one target\n",
        " \n",
        "  zipped = tf.data.Dataset.zip((mnist.shuffle(1000), \n",
        "                                mnist.shuffle(1000)))\n",
        "  zipped = zipped.map(lambda d1, d2: (d1[0], d2[0], d1[1] + d2[1] >= 5))\n",
        "  zipped = zipped.map(lambda d1,d2,t: (d1,d2, tf.cast(t, tf.int32)))\n",
        "\n",
        "  data = zipped.batch(batch_size)\n",
        "  # prefetch\n",
        "  data = data.prefetch(tf.data.AUTOTUNE)\n",
        "  return data\n",
        "\n",
        "#apply the preparation function to the datasets respectively \n",
        "train_ds_problem1 = prepare_mnist_data_problem1(train_ds, 32)\n",
        "val_ds_problem1 = prepare_mnist_data_problem1(val_ds, 32)"
      ],
      "metadata": {
        "id": "cu6QEg08F9oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_mnist_data_problem2(mnist, batch_size, y_value):\n",
        "    \"\"\"\n",
        "  Preparation of the data set\n",
        "  Args: \n",
        "    batch_size(int): size of the batches used for the training\n",
        "    y_value(int):  Difference between the two images\n",
        "  Returns: \n",
        "    preprocessed data set\n",
        "  \"\"\"\n",
        "  #convert data from uint8 to float32\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #flatten the images into vectors\n",
        "  mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "  #create tuple with two data points and one target\n",
        " \n",
        "  zipped = tf.data.Dataset.zip((mnist.shuffle(1000), \n",
        "                                mnist.shuffle(1000)))\n",
        "  zipped = zipped.map(lambda d1, d2: (d1[0], d2[0], d1[1] - d2[1] == y_value))\n",
        "  zipped = zipped.map(lambda d1,d2,t: (d1,d2, tf.cast(t, tf.int32)))\n",
        "\n",
        "  data = zipped.batch(batch_size)\n",
        "  # prefetch\n",
        "  data = data.prefetch(tf.data.AUTOTUNE)\n",
        "  return data\n",
        "\n",
        "\n",
        "train_ds_problem2 = prepare_mnist_data_problem2(train_ds, 32, 5)\n",
        "val_ds_problem2 = prepare_mnist_data_problem2(val_ds, 32, 5)"
      ],
      "metadata": {
        "id": "pQAwDzmjGV3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "class MyModel(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  The model we are using for predicting the outcome \n",
        "  \"\"\" \n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializing the model \n",
        "    \"\"\"\n",
        "    #inherit from parent class\n",
        "    super().__init__()\n",
        "\n",
        "    #metrics we want to display\n",
        "    self.metrics_list = [tf.keras.metrics.BinaryAccuracy(), \n",
        "                         tf.keras.metrics.Mean(name = 'loss')]\n",
        "                \n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
        "        \n",
        "    self.out_layer = tf.keras.layer.Dense(1,activation=tf.nn.sigmoid)\n",
        "\n",
        "  @tf.function \n",
        "  def call(self, images):\n",
        "    \"\"\"\n",
        "    how to forward the images through the layer\n",
        "    \"\"\"\n",
        "    img1, img2 = images \n",
        "\n",
        "    img1_x = self.dense1(img1)\n",
        "    img1_x = self.dense2(img1_x)\n",
        "\n",
        "    img2_x = self.dense1(img2)\n",
        "    img2_x = self.dense2(img2_x)\n",
        "\n",
        "    combined = tf.concat([img1_x, img2_x], axis = 1)\n",
        "    out = self.out_layer(combined)\n",
        "\n",
        "    return out\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metric(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    img1, img2, t = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self((img1, img2), training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    prediction = self(x, training = False)\n",
        "    loss = self.loss_function(t, prediction)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "dUQ1Kr1dAPXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import pprint \n",
        "\n",
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer): \n",
        "  for epoch in range(epoch): \n",
        "    print(f\"Epoch {epoch}: \")\n",
        "\n",
        "    for data in tqdm.tqdm(train_ds, position = 0, leave = True):\n",
        "\n",
        "      metrics = model.train_step(data)\n",
        "\n",
        "      with train_summary_writer.as_default(): \n",
        "        for metric in model.metrics: \n",
        "          tf.summary.scalar()\n",
        "\n",
        "      \n",
        "      model.reset_metrics()\n",
        "  \n",
        "    for data in val_ds:\n",
        "      metrics = model.test_step(data)\n",
        "\n",
        "      with test_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "          tf.summary.scalar()\n",
        "\n",
        "\n",
        "      model.reset_metric()\n",
        "      "
      ],
      "metadata": {
        "id": "AI2WJpkkMhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_function(problem, optimizer): \n"
      ],
      "metadata": {
        "id": "N_vTn1cZMPKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}