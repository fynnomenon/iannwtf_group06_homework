{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucIYjbQEqeYa",
        "outputId": "2a3c4dcb-2fd2-44ec-d273-31fdb4d6f4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import datetime\n",
        "import tqdm\n",
        "import pprint\n",
        "\n",
        "# in a notebook, load the tensorboard extension, not needed for scripts\n",
        "%load_ext tensorboard\n",
        "\n",
        "# load the mnist dataset\n",
        "# (train_ds, test_ds) = tfds.load('Cifar10', split=['train', 'test'], as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kaTVli4hqw_-"
      },
      "outputs": [],
      "source": [
        "# def prepare(batch_size, data, augmentation = None):\n",
        "#   data = data.map(lambda img, target: (tf.cast(img, tf.float32), tf.cast(target, tf.int32)))\n",
        "#   #bring the values into a range between -1 and 1\n",
        "#   data = data.map(lambda img, target: ((img/255), target))\n",
        "#   data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "#   data = data.cache()\n",
        "#   data = data.shuffle(1000)\n",
        "#   data = data.batch(batch_size)\n",
        "\n",
        "#   if augmentation:\n",
        "#     #data = data.map(lambda img, target: (tf.image.adjust_brightness(img, delta = 0.1), target))\n",
        "#     #data = data.map(lambda img, target: (tf.image.adjust_saturation(img, saturation_factor = 0.2, name = None), target))\n",
        "#     #data = data.map(lambda img, target: (tf.image.flip_left_right(img), target))\n",
        "    \n",
        "\n",
        "#   data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#   return data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(batch_size, augmentation = None): \n",
        "    '''\n",
        "    prepare the dataset to have one-hot-vectors and values between -1 and 1\n",
        "    '''\n",
        "    train_ds, test_ds = tfds.load('Cifar10', split=['train', 'test'], as_supervised=True)\n",
        "    \n",
        "    prep_func = lambda img, t: (tf.cast(img, tf.float32)/255.,tf.one_hot(tf.cast(t,tf.int32), depth=10))\n",
        "    train_ds = train_ds.map(prep_func).cache().shuffle(1028).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_ds = test_ds.map(prep_func).cache().shuffle(1028).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "        tf.keras.layers.RandomRotation(0.3)])\n",
        "    \n",
        "    if augmentation:\n",
        "        train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training = True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "    return train_ds, test_ds"
      ],
      "metadata": {
        "id": "ZvTUfKNHwZ0-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558,
          "referenced_widgets": [
            "393aa53699fc40c0a9dffaba14ed16ab",
            "0b939ee4f53c4006b06fb6f6f4736aad",
            "ed9ee3d36fa94b4e93e2129a17af45bf",
            "1874c3bb8f61408cb08082f9e777c91d",
            "8faa19af46de4ff786b197336c1c0f36",
            "1aebc12029ab4463ad20261903e3daa0",
            "4c80b4631d284d5987a44633a0e3d2b3",
            "b6409ef979de40c9bc2e2f36c423f9f9",
            "590ac92c22a44dddb86277fcb543ff50",
            "490e4ceb416146068297d222d5428fee",
            "b0486aee59de46c9b1e98b741d4aadbb",
            "d8e6f809e74f4e718fcfe531ab26c622",
            "e10d96d6b53140e48a92c9a817527ca9",
            "090f88c7aabb41b7b8943ec769be3624",
            "404f470d1f6a4f5089b51b8222aa8136",
            "a290124c799d4a3180a0efc96e01a784",
            "3a33ec7bd66b4fc6afc028c918e54ccf",
            "380cd1546b6e458fa4c4cad5e066795f",
            "a12f129b7959484490214b6035ed125a",
            "11c1dec718084fd4818b2ccf4b87a7df",
            "c1313aa7a0094e5aa4e5c25048493b54",
            "80fdb1cb0e534f6dbbfb8f663844b767",
            "c41cade116044afdb415f5e3f96a9c61",
            "3eab4cfc9ae84533aed8d73bbb4e6b88",
            "473f77e949e645da8e82a51200daf4ba",
            "4c8fec2f85144dacb0998b288ff020ca",
            "faf5b300afc942a08b46e6921ca3a430",
            "b412fb8a960b4435a0e75d4898383779",
            "44c128a24ac541a9af1188c09c5230f5",
            "91fc022dad7a42378feadae4bafa05f8",
            "ff127b74a5ed4eac9ca932c190b8369f",
            "fb8127eb56af437ca86bb4c7bfeccb1e",
            "d05ecb6452714b88b75d040d69b586cc",
            "3e03b422c14c47a1bcad17b9c4b29e7d",
            "f5bf9cddcdc144c9a041f8d868e932ed",
            "5bb8aca136e0499a8a5053e9b892c7e0",
            "aa0b505512b04f40b21a1ca7b7e33532",
            "d6bb3997ee9442528fc02a14cdc7b432",
            "9e8b484ea33d457aa57b5d1e2a9bcb6c",
            "afc616a850164ddf93cebdae94a14e7f",
            "7bc2d492657941eea4ec8bd2721e3c15",
            "aef55c9c960e4874bd294c9cebf41c02",
            "074570bc8e624156962530f7fad946a2",
            "3f2ce3a511a64885af9b385a24eaf67b",
            "31ae689830774f5f9d6e4f62957cec11",
            "fd4a7a36c78d477da897ba7e9847765f",
            "b53deac599844f4fa59675ea0e7af798",
            "62b38f6c23144132ae80880c62444d2f",
            "576bbfd4e2eb4710b7906cbcacb3473e",
            "9fe4a8709e314db894a75d909ce1a548",
            "3a3b1ad94ec449e1a22eb963121072ed",
            "abb05461a7de42e5be981927ecf9943d",
            "899e07bedee34568958bff94b82ad8d3",
            "84fe84dd9c734e7fa39d4fed3623d524",
            "3a936124dc54433faa00fa11ae9011d9",
            "befdc1ff921141818425580c36bbef53",
            "ee3ad611896841ff92037b5e66a79243",
            "106e5ba4f700495599733e6844475b7d",
            "4b8dd74c681547cdbe8b839b513e0f0a",
            "70d1a4abbdb540e7b09d0e7b99466764",
            "563362847e574d0987703cf920154dd9",
            "ad364ebfddb4406c9c4f738a4d721381",
            "075838312399459eb435470f78626a74",
            "31d2b980053c4730bf682d8346ca05ad",
            "85d885ce07ec4849bb7bb07ff34c891a",
            "37159a7aa5c34e8484cc5aedfdbd2b98",
            "6eca53fea2864fd58ac4c7c4ecf1da09",
            "82455e383d9543ffabcaeb14fcdae14c",
            "7f79e27c537b4836b63cf43ceb805b17",
            "b2619af7518242d1a5a68d1f79e62871",
            "eabb6150e81149dfac9d673f9082bb88",
            "0f3b635894524fbf8fbdc6ea177d13c2",
            "15e82c5ac2e14fb18fab550f699581a3",
            "f375c9b4a9754a778ec49908c1c71c61",
            "ea7884f3a1f1480a90dcbd1b41e84de0",
            "8594f751ae5441ea8fc730e67e8bf95d",
            "926b8dfd00de482da4f3a9280310397c",
            "712b22932d7a4d4d87114fc77cfb2640",
            "9dd52622f2f144c9ab6f5d7b1c4d8a44",
            "94c6a688fc594e8691d41fb43fe268f0",
            "0a2138af9f404f9c8c07015a8185dfbf",
            "362528f0db7d4d8aa93b63039dd18b26",
            "6637b2a44b664bb7b964e1b12dfcccc8",
            "b1eb48b103a84d53a4cd449167b03322",
            "b8eded99ccf64107bdb4d364455cb5e2",
            "a6c0132a403d4f3983a8d21dc264e9e1",
            "8ed61a4c98d140ad85f7526cc00a28d4",
            "30866c46772f4c07a77f675e6547ebc1"
          ]
        },
        "id": "Tph7-OSnty8j",
        "outputId": "79f530bc-8014-4793-f4d1-038801e36993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to ~/tensorflow_datasets/cifar10/3.0.2...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "393aa53699fc40c0a9dffaba14ed16ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8e6f809e74f4e718fcfe531ab26c622"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c41cade116044afdb415f5e3f96a9c61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e03b422c14c47a1bcad17b9c4b29e7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31ae689830774f5f9d6e4f62957cec11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteUACSWQ/cifar10-train.tfrecord*...:   0%|          | 0/…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "befdc1ff921141818425580c36bbef53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eca53fea2864fd58ac4c7c4ecf1da09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteUACSWQ/cifar10-test.tfrecord*...:   0%|          | 0/1…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "712b22932d7a4d4d87114fc77cfb2640"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cifar10 downloaded and prepared to ~/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train, test = prepare_data(32, True)\n",
        "\n",
        "train = list(train.take(1))\n",
        "test = list(test.take(1))\n",
        "\n",
        "for img in [train, test]: \n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(img[0][0][0])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DzmKVfZxFw7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class BasicConv(tf.keras.Model):\n",
        "  '''\n",
        "  create a small network with 2 blocks each having 2 layers and starting with 32 filters in the first layer\n",
        "  '''\n",
        "  def __init__(self, optimizer = tf.keras.optimizers.Adam(), L2_norm = 0, L1_norm = 0, dropout_rate=0, batch_norm = None,label_smoothing=0):\n",
        "    super(BasicConv, self).__init__()\n",
        "\n",
        "    if L2_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L2(L2_norm)\n",
        "    elif L1_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L1(L1_norm)\n",
        "    else:\n",
        "      kernel_regularizer=None\n",
        "\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    if self.dropout_rate: \n",
        "      self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n",
        "\n",
        "    self.metrics_list = [tf.keras.metrics.Mean(name = \"loss\"),\n",
        "                         tf.keras.metrics.Mean(name = \"frobenius\"),\n",
        "                         tf.keras.metrics.CategoricalAccuracy(name = \"acc\")]\n",
        "\n",
        "    \n",
        "    self.layer_list =  [tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                        tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.GlobalAvgPool2D(),\n",
        "                        tf.keras.layers.Dense(10, activation = 'softmax')]\n",
        "                        \n",
        "    if batch_norm: \n",
        "      self.layer_list.insert(1,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(3,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(6,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(8,tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "  def call(self, x , training = False):\n",
        "    for layer in self.layer_list[:-1]:\n",
        "      x = layer(x)\n",
        "      if self.dropout_rate:\n",
        "        x = self.dropout_layer(x,training)\n",
        "    return self.layer_list[-1](x)\n",
        "\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l1(x)\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l2(x)\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l3(x)\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l4(x)\n",
        "    # x = self.global_pool(x)\n",
        "    # x = self.out(x)\n",
        "\n",
        "  \n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metrics(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  def compute_frobenius(self): \n",
        "    frobenius_norm = tf.zeros((1,))\n",
        "    for var in self.trainable_variables: \n",
        "      frobenius_norm += tf.norm(var, ord = \"euclidean\")\n",
        "    return frobenius_norm\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jugHL2N0SV39"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class DeepConv(tf.keras.Model):\n",
        "  '''\n",
        "  create a small network with 2 blocks each having 2 layers and starting with 32 filters in the first layer\n",
        "  '''\n",
        "  def __init__(self, optimizer = tf.keras.optimizers.Adam(), L2_norm = 0, L1_norm = 0, dropout_rate=0, batch_norm = None,label_smoothing=0):\n",
        "    super(DeepConv, self).__init__()\n",
        "\n",
        "    if L2_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L2(L2_norm)\n",
        "    elif L1_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L1(L1_norm)\n",
        "    else:\n",
        "      kernel_regularizer=None\n",
        "\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    if self.dropout_rate: \n",
        "      self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n",
        "\n",
        "    self.metrics_list = [tf.keras.metrics.Mean(name = \"loss\"),\n",
        "                         tf.keras.metrics.Mean(name = \"frobenius\"),\n",
        "                         tf.keras.metrics.CategoricalAccuracy(name = \"acc\")]\n",
        "\n",
        "    \n",
        "    self.layer_list =  [tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                        tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                        tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = kernel_regularizer),\n",
        "                        tf.keras.layers.GlobalAvgPool2D(),\n",
        "                        tf.keras.layers.Dense(10, activation = 'softmax')]\n",
        "                        \n",
        "    if batch_norm: \n",
        "      self.layer_list.insert(1,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(3,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(6,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(8,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(11,tf.keras.layers.BatchNormalization())\n",
        "      self.layer_list.insert(13,tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, x , training = False):\n",
        "    for layer in self.layer_list[:-1]:\n",
        "      x = layer(x)\n",
        "      if self.dropout_rate:\n",
        "        x = self.dropout_layer(x,training)\n",
        "    return self.layer_list[-1](x)\n",
        "\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l1(x)\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l2(x)\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l3(x)\n",
        "    # x = self.dropout_layer(x, training) if self.dropout_rate else self.l4(x)\n",
        "    # x = self.global_pool(x)\n",
        "    # x = self.out(x)\n",
        "\n",
        "  \n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "\n",
        "  def reset_metrics(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  def compute_frobenius(self): \n",
        "    frobenius_norm = tf.zeros((1,))\n",
        "    for var in self.trainable_variables: \n",
        "      frobenius_norm += tf.norm(var, ord = \"euclidean\")\n",
        "    return frobenius_norm\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Bn-E6U66EO"
      },
      "outputs": [],
      "source": [
        "class ResidualConnectedCNNLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, depth, L1_norm, L2_norm):\n",
        "    super(ResidualConnectedCNNLayer, self).__init__()\n",
        "\n",
        "    if L2_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L2(L2_norm)\n",
        "    elif L1_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L1(L1_norm)\n",
        "    else:\n",
        "      kernel_regularizer=None\n",
        "\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=depth, kernel_size=3, padding='same', activation='relu', kernel_regularizer = kernel_regularizer)\n",
        "\n",
        "  def call(self, x):\n",
        "    c = self.conv(x)\n",
        "    #residual connection \n",
        "    x = c+x\n",
        "    return x\n",
        "\n",
        "class ResidualConnectedCNNBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, depth, layers, L1_norm = 0, L2_norm = 0, dropout_rate = 0):\n",
        "    super(ResidualConnectedCNNBlock, self).__init__()\n",
        "\n",
        "    if L2_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L2(L2_norm)\n",
        "    elif L1_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L1(L1_norm)\n",
        "    else:\n",
        "      kernel_regularizer=None\n",
        "\n",
        "\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    if self.dropout_rate:\n",
        "      self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    #depth increases in the end of the network (cannot add them with different sizes)\n",
        "    #specify the depth such that they get the same depth \n",
        "    self.deeper_layer = tf.keras.layers.Conv2D(filters=depth, kernel_size=3, padding='same', activation='relu', kernel_regularizer = kernel_regularizer)\n",
        "    #list with many residual connected cnn layer\n",
        "    self.layers = [ResidualConnectedCNNLayer(depth, L1_norm, L2_norm) for _ in range(layers)]\n",
        "\n",
        "  def call(self, x, training = False):\n",
        "    #bring it to the correct depth \n",
        "    x = self.deeper_layer(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "      if self.dropout_rate: \n",
        "        x = self.dropout_layer(x, training)\n",
        "    return x\n",
        "\n",
        "\n",
        "class ResidualConnectedCNN(tf.keras.Model):\n",
        "  def __init__(self, optimizer = tf.keras.optimizers.Adam(),L1_norm = 0, L2_norm = 0, batch_norm = False,dropout_rate=0,label_smoothing=0):\n",
        "    super(ResidualConnectedCNN, self).__init__()\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n",
        "\n",
        "    self.metrics_list = [tf.keras.metrics.Mean(name = \"loss\"),\n",
        "                         tf.keras.metrics.Mean(name = \"total_frobenius_norm\"),\n",
        "                         tf.keras.metrics.CategoricalAccuracy(name = \"acc\")]\n",
        "\n",
        "    # self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
        "    # self.accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"acc\")\n",
        "    # self.frobenius_metric = tf.keras.metrics.Mean(name = \"total_frobenius_norm\")\n",
        "\n",
        "    self.block_list = [ResidualConnectedCNNBlock(24,4,L1_norm = L1_norm, L2_norm = L2_norm,dropout_rate = dropout_rate),\n",
        "                       tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                       ResidualConnectedCNNBlock(48,4, L1_norm = L1_norm, L2_norm = L2_norm,dropout_rate = dropout_rate),\n",
        "                       tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
        "                       ResidualConnectedCNNBlock(96,4, L1_norm = L1_norm, L2_norm = L2_norm,dropout_rate = dropout_rate),\n",
        "                       tf.keras.layers.GlobalAvgPool2D()]\n",
        "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    if batch_norm: \n",
        "      self.block_list.insert(1,tf.keras.layers.BatchNormalization())\n",
        "      self.block_list.insert(4,tf.keras.layers.BatchNormalization())\n",
        "      self.block_list.insert(7,tf.keras.layers.BatchNormalization())\n",
        "    \n",
        "\n",
        "  def call(self,x):\n",
        "    for block in self.block_list:\n",
        "      x = block(x)\n",
        "      \n",
        "    return self.out(x)\n",
        "\n",
        "  def compute_frobenius(self): \n",
        "    frobenius_norm = tf.zeros((1,))\n",
        "    for var in self.trainable_variables: \n",
        "      frobenius_norm += tf.norm(var, ord = \"euclidean\")\n",
        "    return frobenius_norm\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "  def reset_metrics(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZX0lClg5-eC"
      },
      "outputs": [],
      "source": [
        "class DenselyConnectedCNNLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_filter, L1_norm = 0, L2_norm = 0): \n",
        "    super(DenselyConnectedCNNLayer, self).__init__()\n",
        "\n",
        "    if L2_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L2(L2_norm)\n",
        "    elif L1_norm:\n",
        "      kernel_regularizer=tf.keras.regularizers.L1(L1_norm)\n",
        "    else:\n",
        "      kernel_regularizer=None\n",
        "  \n",
        "\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = num_filter, kernel_size = 3, padding = \"same\", activation = \"relu\", kernel_regularizer = kernel_regularizer)\n",
        "\n",
        "  def call(self,x): \n",
        "    c = self.conv(x)\n",
        "    #concatenate at last layer - get long stack of all earlier layer\n",
        "    x = tf.concat((x,c), axis = -1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DenselyConnectedCNNBlock(tf.keras.layers.Layer):\n",
        "  #take all conv layers and create a block we take for one convolution\n",
        "  def __init__(self, num_filters, layers, L1_norm = 0, L2_norm = 0, dropout_rate = 0):\n",
        "    super(DenselyConnectedCNNBlock, self).__init__()\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    if self.dropout_rate:\n",
        "      self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.layers = [DenselyConnectedCNNLayer(num_filters, L1_norm, L2_norm) for _ in range(layers)]\n",
        "\n",
        "\n",
        "  def call(self, x, training = False): \n",
        "    #take input and pump it through all layers we have \n",
        "    for l in self.layers:\n",
        "      x = l(x)\n",
        "      if self.dropout_rate: \n",
        "        x = self.dropout_layer(x, training)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DenselyConnectedCNN(tf.keras.Model):\n",
        "  def __init__(self, optimizer = tf.keras.optimizers.Adam(), L1_norm=0,L2_norm=0,batch_norm = False,dropout_rate=0,label_smoothing=0):\n",
        "    super(DenselyConnectedCNN, self).__init__()\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n",
        "\n",
        "    self.metrics_list = [tf.keras.metrics.Mean(name = \"loss\"),\n",
        "                         tf.keras.metrics.Mean(name = \"total_frobenius_norm\"),\n",
        "                         tf.keras.metrics.CategoricalAccuracy(name = \"acc\")]\n",
        "\n",
        "    #4 layers with each of 24 kernels -> get 4 layer in 28*28 resolution \n",
        "    self.block_list = [DenselyConnectedCNNBlock(24,4,L1_norm = L1_norm, L2_norm = L2_norm,dropout_rate = dropout_rate),\n",
        "                       tf.keras.layers.MaxPooling2D(),\n",
        "                       DenselyConnectedCNNBlock(24,4,L1_norm = L1_norm, L2_norm = L2_norm,dropout_rate = dropout_rate),\n",
        "                       tf.keras.layers.MaxPooling2D(),\n",
        "                       DenselyConnectedCNNBlock(24,4, L1_norm = L1_norm, L2_norm = L2_norm,dropout_rate = dropout_rate),\n",
        "                       tf.keras.layers.GlobalAvgPool2D()]\n",
        "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    \n",
        "    if batch_norm: \n",
        "      self.block_list.insert(1,tf.keras.layers.BatchNormalization())\n",
        "      self.block_list.insert(4,tf.keras.layers.BatchNormalization())\n",
        "      self.block_list.insert(7,tf.keras.layers.BatchNormalization())\n",
        "    \n",
        "  def call(self,x):\n",
        "    for block in self.block_list:\n",
        "      x = block(x)\n",
        "      \n",
        "    return self.out(x)\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "\n",
        "  def compute_frobenius(self): \n",
        "    frobenius_norm = tf.zeros((1,))\n",
        "    for var in self.trainable_variables: \n",
        "      frobenius_norm += tf.norm(var, ord = \"euclidean\")\n",
        "    return frobenius_norm\n",
        "\n",
        "  def reset_metrics(self):\n",
        "    for metric in self.metrics: \n",
        "      metric.reset_states()\n",
        "\n",
        "  @tf.function \n",
        "  def train_step(self, data):\n",
        "    x, t = data\n",
        "    #print(x.shape)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(x, training = True)\n",
        "      loss = self.loss_function(t, output)\n",
        "    \n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    x, t = data\n",
        "\n",
        "    output = self(x, training = False)\n",
        "    loss = self.loss_function(t, output)\n",
        "\n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(self.compute_frobenius())\n",
        "    \n",
        "    for metric in self.metrics[2:]:\n",
        "          metric.update_state(t,output)\n",
        "\n",
        "    return {metric.name: metric.result() for metric in self.metrics}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iPmlmTcO2z_z"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer): \n",
        "  #iterate over given amount of epochs\n",
        "  for epoch in range(epochs): \n",
        "    print(f\"Epoch {epoch}: \")\n",
        "\n",
        "    for data in tqdm.tqdm(train_ds, position=0, leave = True): \n",
        "\n",
        "      metrics = model.train_step(data)\n",
        "\n",
        "      with train_summary_writer.as_default(): \n",
        "        for metric in model.metrics: \n",
        "          tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "\n",
        "    # print the metrics\n",
        "    print([f\"Train {key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "    \n",
        "\n",
        "    #reset metric \n",
        "    model.reset_metrics()\n",
        "  \n",
        "    #evaluation on validation set\n",
        "    for data in val_ds:\n",
        "      metrics = model.test_step(data)\n",
        "\n",
        "      with val_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "          tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "      \n",
        "    # print the metrics\n",
        "    print([f\"Test {key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
        "\n",
        "    #reset metric\n",
        "    model.reset_metrics()\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLv9Kyl-0WHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI2WJpkkMhEO"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer): \n",
        "    '''\n",
        "    create the training loop where the model is trained on the train dataset and afterwards tested on the validation dataset\n",
        "    '''\n",
        "    hist = {\"train_loss\":[],\"train_acc\":[],\"test_loss\":[], \"test_acc\":[]}\n",
        "    #iterate over given amount of epochs\n",
        "    for epoch in range(epochs): \n",
        "        print(f\"Epoch {epoch}: \")\n",
        "\n",
        "        #train on all batches of the training data\n",
        "        for data in tqdm.tqdm(train_ds, position = 0, leave = True):\n",
        "            metrics = model.train_step(data)\n",
        "\n",
        "            with train_summary_writer.as_default(): \n",
        "                for metric in model.metrics: \n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics and add to history element\n",
        "        for key, value in metrics.items():\n",
        "            hist[f\"train_{key}\"].append(value.numpy())\n",
        "            print(f\"train_{key}: {value.numpy()}\")\n",
        "\n",
        "        #reset metric \n",
        "        model.reset_metrics()\n",
        "\n",
        "        #evaluation on validation set\n",
        "        for data in val_ds:\n",
        "            metrics = model.test_step(data)\n",
        "\n",
        "            with val_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics and add to history element\n",
        "        for key, value in metrics.items():\n",
        "            hist[f\"test_{key}\"].append(value.numpy())\n",
        "            print(f\"test_{key}: {value.numpy()}\")\n",
        "\n",
        "        #reset metric\n",
        "        model.reset_metric()\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    return hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gayZ0E_577pM"
      },
      "outputs": [],
      "source": [
        "def create_summary_writers(config_name):\n",
        "  '''\n",
        "  create the summary writer to have access to the metrics of the model \n",
        "  '''\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  train_log_path = f\"logs/submission/{config_name}/{current_time}/train\"\n",
        "  val_log_path = f\"logs/submission/{config_name}/{current_time}/val\"\n",
        "\n",
        "  # log writer\n",
        "  train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "  val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "    \n",
        "  return train_summary_writer, val_summary_writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vvjxWFUH4JTm"
      },
      "outputs": [],
      "source": [
        "def run(model,num_epochs=15,save=True,load=False,config_name = f'RUN',augmentation=False):\n",
        "  '''\n",
        "  run the model and specify the number of epochs, batch_size and prepare the training and testing data\n",
        "  '''\n",
        "\n",
        "  ### Hyperparameters\n",
        "  num_epochs = num_epochs\n",
        "  batch_size = 32\n",
        "  train,test = prepare_data(batch_size,augmentation=augmentation)\n",
        "  \n",
        "  \n",
        "  train_summary_writer, val_summary_writer = create_summary_writers(config_name)\n",
        "\n",
        "\n",
        "  if load:\n",
        "    model.load_weights(f\"saved_model_{config_name}\");\n",
        "\n",
        "  hist = training_loop(model,train,test,num_epochs,train_summary_writer, val_summary_writer)\n",
        "  if save:\n",
        "    model.save_weights(f\"saved_model_{config_name}\", save_format=\"tf\")\n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "  fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
        "\n",
        "  ax[0].plot(hist['train_acc'], label='train accuracy')\n",
        "  ax[0].plot(hist['test_acc'], label='test accuracy')\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(hist['train_loss'], label='train loss')\n",
        "  ax[1].plot(hist['test_loss'], label='test loss')\n",
        "  ax[1].legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta768kIo4TMD"
      },
      "outputs": [],
      "source": [
        "myMod1 = BasicConv(tf.keras.optimizers.Adam(), L1_norm = 0.4, batch_norm = True, dropout_rate=0,label_smoothing=0.1)\n",
        "\n",
        "print(\"Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\")\n",
        "run(myMod1,1,config_name = f'BASIC1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JraV2hr1UCVg",
        "outputId": "298fdcb0-e6b4-47d9-f2bd-e85eabd92546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\n",
            "Epoch 0: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:40<00:00, 38.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.7550135850906372', 'Train frobenius: 105.85831451416016', 'Train acc: 0.36875998973846436']\n",
            "['Test loss: 2.1551434993743896', 'Test frobenius: 113.14630126953125', 'Test acc: 0.31619998812675476']\n",
            "\n",
            "\n",
            "Epoch 1: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:37<00:00, 42.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.5130109786987305', 'Train frobenius: 121.00579071044922', 'Train acc: 0.46272000670433044']\n",
            "['Test loss: 1.5307250022888184', 'Test frobenius: 128.65985107421875', 'Test acc: 0.47110000252723694']\n",
            "\n",
            "\n",
            "Epoch 2: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:36<00:00, 43.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.395881175994873', 'Train frobenius: 136.17257690429688', 'Train acc: 0.5094000101089478']\n",
            "['Test loss: 1.5533561706542969', 'Test frobenius: 143.3436737060547', 'Test acc: 0.4758000075817108']\n",
            "\n",
            "\n",
            "Epoch 3: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.3191763162612915', 'Train frobenius: 150.0686798095703', 'Train acc: 0.5418400168418884']\n",
            "['Test loss: 1.5558875799179077', 'Test frobenius: 156.5908966064453', 'Test acc: 0.46959999203681946']\n",
            "\n",
            "\n",
            "Epoch 4: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.2640129327774048', 'Train frobenius: 162.60523986816406', 'Train acc: 0.5607799887657166']\n",
            "['Test loss: 1.3597612380981445', 'Test frobenius: 168.3681182861328', 'Test acc: 0.5310999751091003']\n",
            "\n",
            "\n",
            "Epoch 5: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.220068335533142', 'Train frobenius: 173.98834228515625', 'Train acc: 0.5796399712562561']\n",
            "['Test loss: 1.3022205829620361', 'Test frobenius: 179.57029724121094', 'Test acc: 0.5602999925613403']\n",
            "\n",
            "\n",
            "Epoch 6: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.1888571977615356', 'Train frobenius: 184.6574249267578', 'Train acc: 0.5904399752616882']\n",
            "['Test loss: 1.2219041585922241', 'Test frobenius: 189.62399291992188', 'Test acc: 0.5821999907493591']\n",
            "\n",
            "\n",
            "Epoch 7: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.168203353881836', 'Train frobenius: 194.5062713623047', 'Train acc: 0.599560022354126']\n",
            "['Test loss: 1.1712971925735474', 'Test frobenius: 199.17962646484375', 'Test acc: 0.607200026512146']\n",
            "\n",
            "\n",
            "Epoch 8: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:38<00:00, 40.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.1343902349472046', 'Train frobenius: 203.6898193359375', 'Train acc: 0.6114799976348877']\n",
            "['Test loss: 1.1997822523117065', 'Test frobenius: 208.17189025878906', 'Test acc: 0.5910999774932861']\n",
            "\n",
            "\n",
            "Epoch 9: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 44.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.1140711307525635', 'Train frobenius: 212.37527465820312', 'Train acc: 0.6215999722480774']\n",
            "['Test loss: 1.1381086111068726', 'Test frobenius: 216.43252563476562', 'Test acc: 0.6118000149726868']\n",
            "\n",
            "\n",
            "Epoch 10: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.1031643152236938', 'Train frobenius: 220.46644592285156', 'Train acc: 0.6251199841499329']\n",
            "['Test loss: 1.191435694694519', 'Test frobenius: 224.5112762451172', 'Test acc: 0.5985000133514404']\n",
            "\n",
            "\n",
            "Epoch 11: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.088219404220581', 'Train frobenius: 228.24896240234375', 'Train acc: 0.6302800178527832']\n",
            "['Test loss: 1.1770625114440918', 'Test frobenius: 231.92259216308594', 'Test acc: 0.605400025844574']\n",
            "\n",
            "\n",
            "Epoch 12: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0719906091690063', 'Train frobenius: 235.7054901123047', 'Train acc: 0.6353200078010559']\n",
            "['Test loss: 1.1437644958496094', 'Test frobenius: 239.2310333251953', 'Test acc: 0.6169000267982483']\n",
            "\n",
            "\n",
            "Epoch 13: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0610755681991577', 'Train frobenius: 242.76321411132812', 'Train acc: 0.6414200067520142']\n",
            "['Test loss: 1.1764400005340576', 'Test frobenius: 246.28057861328125', 'Test acc: 0.6075000166893005']\n",
            "\n",
            "\n",
            "Epoch 14: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 44.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0565773248672485', 'Train frobenius: 249.72731018066406', 'Train acc: 0.643559992313385']\n",
            "['Test loss: 1.2693469524383545', 'Test frobenius: 253.1945343017578', 'Test acc: 0.5710999965667725']\n",
            "\n",
            "\n",
            "Epoch 15: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.03623366355896', 'Train frobenius: 256.5106201171875', 'Train acc: 0.6497600078582764']\n",
            "['Test loss: 1.1467669010162354', 'Test frobenius: 259.6302795410156', 'Test acc: 0.6244999766349792']\n",
            "\n",
            "\n",
            "Epoch 16: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 44.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0326321125030518', 'Train frobenius: 262.9592590332031', 'Train acc: 0.6517000198364258']\n",
            "['Test loss: 1.1464242935180664', 'Test frobenius: 265.9969787597656', 'Test acc: 0.6233000159263611']\n",
            "\n",
            "\n",
            "Epoch 17: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0164204835891724', 'Train frobenius: 269.3172912597656', 'Train acc: 0.6579599976539612']\n",
            "['Test loss: 1.0736463069915771', 'Test frobenius: 272.2890930175781', 'Test acc: 0.6389999985694885']\n",
            "\n",
            "\n",
            "Epoch 18: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:36<00:00, 42.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0191980600357056', 'Train frobenius: 275.4630126953125', 'Train acc: 0.6569799780845642']\n",
            "['Test loss: 1.1266512870788574', 'Test frobenius: 278.5024108886719', 'Test acc: 0.6297000050544739']\n",
            "\n",
            "\n",
            "Epoch 19: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0045346021652222', 'Train frobenius: 281.5091857910156', 'Train acc: 0.6632400155067444']\n",
            "['Test loss: 1.093546748161316', 'Test frobenius: 284.3650207519531', 'Test acc: 0.6340000033378601']\n",
            "\n",
            "\n",
            "Epoch 20: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 44.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 1.0049397945404053', 'Train frobenius: 287.3288879394531', 'Train acc: 0.6629999876022339']\n",
            "['Test loss: 1.05251944065094', 'Test frobenius: 290.012939453125', 'Test acc: 0.6518999934196472']\n",
            "\n",
            "\n",
            "Epoch 21: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9958073496818542', 'Train frobenius: 292.9557189941406', 'Train acc: 0.6650599837303162']\n",
            "['Test loss: 1.0390172004699707', 'Test frobenius: 295.7654724121094', 'Test acc: 0.6473000049591064']\n",
            "\n",
            "\n",
            "Epoch 22: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9868081212043762', 'Train frobenius: 298.6612243652344', 'Train acc: 0.6683200001716614']\n",
            "['Test loss: 1.0618075132369995', 'Test frobenius: 301.5052795410156', 'Test acc: 0.6460999846458435']\n",
            "\n",
            "\n",
            "Epoch 23: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9770427346229553', 'Train frobenius: 304.17254638671875', 'Train acc: 0.6723600029945374']\n",
            "['Test loss: 1.2082173824310303', 'Test frobenius: 306.90618896484375', 'Test acc: 0.6144000291824341']\n",
            "\n",
            "\n",
            "Epoch 24: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 44.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9736402034759521', 'Train frobenius: 309.6049499511719', 'Train acc: 0.6744199991226196']\n",
            "['Test loss: 0.998740017414093', 'Test frobenius: 312.1927490234375', 'Test acc: 0.6703000068664551']\n",
            "\n",
            "\n",
            "Epoch 25: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:37<00:00, 41.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9706475734710693', 'Train frobenius: 314.71453857421875', 'Train acc: 0.6754000186920166']\n",
            "['Test loss: 1.0897916555404663', 'Test frobenius: 317.2445983886719', 'Test acc: 0.6427000164985657']\n",
            "\n",
            "\n",
            "Epoch 26: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9666680693626404', 'Train frobenius: 319.79705810546875', 'Train acc: 0.6777999997138977']\n",
            "['Test loss: 1.010535717010498', 'Test frobenius: 322.3179016113281', 'Test acc: 0.6603000164031982']\n",
            "\n",
            "\n",
            "Epoch 27: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:36<00:00, 43.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9563980102539062', 'Train frobenius: 324.8113098144531', 'Train acc: 0.6801400184631348']\n",
            "['Test loss: 0.995762050151825', 'Test frobenius: 327.3592224121094', 'Test acc: 0.6717000007629395']\n",
            "\n",
            "\n",
            "Epoch 28: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9530737400054932', 'Train frobenius: 329.92236328125', 'Train acc: 0.6821200251579285']\n",
            "['Test loss: 0.9770217537879944', 'Test frobenius: 332.37310791015625', 'Test acc: 0.6765999794006348']\n",
            "\n",
            "\n",
            "Epoch 29: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9515612721443176', 'Train frobenius: 334.8370361328125', 'Train acc: 0.6797599792480469']\n",
            "['Test loss: 0.9672732353210449', 'Test frobenius: 337.09149169921875', 'Test acc: 0.6794000267982483']\n",
            "\n",
            "\n",
            "Epoch 30: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9493729472160339', 'Train frobenius: 339.5521545410156', 'Train acc: 0.6836599707603455']\n",
            "['Test loss: 0.9899492859840393', 'Test frobenius: 341.8353576660156', 'Test acc: 0.6726999878883362']\n",
            "\n",
            "\n",
            "Epoch 31: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9421850442886353', 'Train frobenius: 344.2629699707031', 'Train acc: 0.6876599788665771']\n",
            "['Test loss: 1.0441731214523315', 'Test frobenius: 346.5698547363281', 'Test acc: 0.6514999866485596']\n",
            "\n",
            "\n",
            "Epoch 32: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9319517016410828', 'Train frobenius: 349.0814208984375', 'Train acc: 0.6906200051307678']\n",
            "['Test loss: 0.9751249551773071', 'Test frobenius: 351.3689270019531', 'Test acc: 0.6717000007629395']\n",
            "\n",
            "\n",
            "Epoch 33: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9337105751037598', 'Train frobenius: 353.66986083984375', 'Train acc: 0.6900399923324585']\n",
            "['Test loss: 1.0404061079025269', 'Test frobenius: 355.9904479980469', 'Test acc: 0.6547999978065491']\n",
            "\n",
            "\n",
            "Epoch 34: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9260905981063843', 'Train frobenius: 358.3252258300781', 'Train acc: 0.690559983253479']\n",
            "['Test loss: 0.9693667888641357', 'Test frobenius: 360.5256652832031', 'Test acc: 0.6794999837875366']\n",
            "\n",
            "\n",
            "Epoch 35: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9298020005226135', 'Train frobenius: 362.73309326171875', 'Train acc: 0.6902400255203247']\n",
            "['Test loss: 0.9934698343276978', 'Test frobenius: 364.7189025878906', 'Test acc: 0.6786999702453613']\n",
            "\n",
            "\n",
            "Epoch 36: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.921915590763092', 'Train frobenius: 367.08056640625', 'Train acc: 0.6934199929237366']\n",
            "['Test loss: 0.9813740253448486', 'Test frobenius: 369.25042724609375', 'Test acc: 0.6790000200271606']\n",
            "\n",
            "\n",
            "Epoch 37: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 44.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9186348915100098', 'Train frobenius: 371.50982666015625', 'Train acc: 0.6940400004386902']\n",
            "['Test loss: 0.9929600954055786', 'Test frobenius: 373.7361145019531', 'Test acc: 0.6754999756813049']\n",
            "\n",
            "\n",
            "Epoch 38: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9214507341384888', 'Train frobenius: 375.86376953125', 'Train acc: 0.6935200095176697']\n",
            "['Test loss: 1.0575815439224243', 'Test frobenius: 377.9451904296875', 'Test acc: 0.6492000222206116']\n",
            "\n",
            "\n",
            "Epoch 39: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:36<00:00, 42.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9148547649383545', 'Train frobenius: 380.18121337890625', 'Train acc: 0.6944199800491333']\n",
            "['Test loss: 0.9259074926376343', 'Test frobenius: 382.2890930175781', 'Test acc: 0.6947000026702881']\n",
            "\n",
            "\n",
            "Epoch 40: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9133115410804749', 'Train frobenius: 384.369873046875', 'Train acc: 0.6940600275993347']\n",
            "['Test loss: 0.9291601181030273', 'Test frobenius: 386.342041015625', 'Test acc: 0.6967999935150146']\n",
            "\n",
            "\n",
            "Epoch 41: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:36<00:00, 43.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9069600105285645', 'Train frobenius: 388.4626770019531', 'Train acc: 0.69896000623703']\n",
            "['Test loss: 0.9341790676116943', 'Test frobenius: 390.5157470703125', 'Test acc: 0.6947000026702881']\n",
            "\n",
            "\n",
            "Epoch 42: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9038214683532715', 'Train frobenius: 392.4776916503906', 'Train acc: 0.6993799805641174']\n",
            "['Test loss: 0.973077654838562', 'Test frobenius: 394.4626159667969', 'Test acc: 0.6844000220298767']\n",
            "\n",
            "\n",
            "Epoch 43: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:36<00:00, 43.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.9045334458351135', 'Train frobenius: 396.56903076171875', 'Train acc: 0.7010800242424011']\n",
            "['Test loss: 0.917121171951294', 'Test frobenius: 398.4297180175781', 'Test acc: 0.6967999935150146']\n",
            "\n",
            "\n",
            "Epoch 44: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.8949931263923645', 'Train frobenius: 400.5504150390625', 'Train acc: 0.7052800059318542']\n",
            "['Test loss: 0.9263420701026917', 'Test frobenius: 402.6233825683594', 'Test acc: 0.6949999928474426']\n",
            "\n",
            "\n",
            "Epoch 45: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.896255612373352', 'Train frobenius: 404.6058349609375', 'Train acc: 0.7046999931335449']\n",
            "['Test loss: 0.9748662114143372', 'Test frobenius: 406.5391540527344', 'Test acc: 0.6807000041007996']\n",
            "\n",
            "\n",
            "Epoch 46: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.895940899848938', 'Train frobenius: 408.4576110839844', 'Train acc: 0.7042999863624573']\n",
            "['Test loss: 1.005096435546875', 'Test frobenius: 410.32183837890625', 'Test acc: 0.6722000241279602']\n",
            "\n",
            "\n",
            "Epoch 47: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:41<00:00, 38.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.8974931836128235', 'Train frobenius: 412.3427734375', 'Train acc: 0.7024199962615967']\n",
            "['Test loss: 0.9681301116943359', 'Test frobenius: 414.0952453613281', 'Test acc: 0.6787999868392944']\n",
            "\n",
            "\n",
            "Epoch 48: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1563/1563 [00:35<00:00, 43.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Train loss: 0.8897035121917725', 'Train frobenius: 416.0400695800781', 'Train acc: 0.7062000036239624']\n",
            "['Test loss: 0.9340289831161499', 'Test frobenius: 417.8730773925781', 'Test acc: 0.6934000253677368']\n",
            "\n",
            "\n",
            "Epoch 49: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 590/1563 [00:13<00:30, 31.53it/s]"
          ]
        }
      ],
      "source": [
        "myMod2 = DeepConv(tf.keras.optimizers.Adam(), L1_norm = 0.2, batch_norm =True, dropout_rate=0.1,label_smoothing=0.01)\n",
        "\n",
        "print(\"Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\")\n",
        "run(myMod2,100,save=False,config_name = f'DEEP1',augmentation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLdmxO8cJxa",
        "outputId": "9be7306d-3a93-45d8-9b6c-3c689b807f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\n",
            "Epoch 0: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:15<00:00, 74.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 1.3874751329421997', 'Train frobenius: 105.62710571289062', 'Train acc: 0.5083733201026917']\n",
            "['Test loss: 1.366857886314392', 'Test frobenius: 112.98002624511719', 'Test acc: 0.5123999714851379']\n",
            "\n",
            "\n",
            "Epoch 1: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:11<00:00, 105.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 1.029874324798584', 'Train frobenius: 119.77091979980469', 'Train acc: 0.649066686630249']\n",
            "['Test loss: 0.9985197186470032', 'Test frobenius: 126.29524230957031', 'Test acc: 0.6552000045776367']\n",
            "\n",
            "\n",
            "Epoch 2: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.27it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.8959730863571167', 'Train frobenius: 132.29934692382812', 'Train acc: 0.7019733190536499']\n",
            "['Test loss: 0.8958852291107178', 'Test frobenius: 138.18714904785156', 'Test acc: 0.6895999908447266']\n",
            "\n",
            "\n",
            "Epoch 3: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 106.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.8186823725700378', 'Train frobenius: 143.89137268066406', 'Train acc: 0.7315733432769775']\n",
            "['Test loss: 0.8550821542739868', 'Test frobenius: 149.46287536621094', 'Test acc: 0.7279999852180481']\n",
            "\n",
            "\n",
            "Epoch 4: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 106.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.7587299942970276', 'Train frobenius: 154.6295928955078', 'Train acc: 0.7545066475868225']\n",
            "['Test loss: 0.786510169506073', 'Test frobenius: 159.6721954345703', 'Test acc: 0.7364000082015991']\n",
            "\n",
            "\n",
            "Epoch 5: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:11<00:00, 103.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.7175527811050415', 'Train frobenius: 164.41725158691406', 'Train acc: 0.7684533596038818']\n",
            "['Test loss: 0.8171215057373047', 'Test frobenius: 168.9420928955078', 'Test acc: 0.7376000285148621']\n",
            "\n",
            "\n",
            "Epoch 6: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.684766948223114', 'Train frobenius: 173.54653930664062', 'Train acc: 0.7832266688346863']\n",
            "['Test loss: 0.8875060081481934', 'Test frobenius: 177.88156127929688', 'Test acc: 0.7003999948501587']\n",
            "\n",
            "\n",
            "Epoch 7: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6553070545196533', 'Train frobenius: 182.16494750976562', 'Train acc: 0.7906133532524109']\n",
            "['Test loss: 0.7044287323951721', 'Test frobenius: 186.335693359375', 'Test acc: 0.769599974155426']\n",
            "\n",
            "\n",
            "Epoch 8: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6223667860031128', 'Train frobenius: 190.37852478027344', 'Train acc: 0.8039199709892273']\n",
            "['Test loss: 0.6786341667175293', 'Test frobenius: 194.3025360107422', 'Test acc: 0.7875999808311462']\n",
            "\n",
            "\n",
            "Epoch 9: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6017967462539673', 'Train frobenius: 198.0560302734375', 'Train acc: 0.8132799863815308']\n",
            "['Test loss: 0.6932611465454102', 'Test frobenius: 201.64453125', 'Test acc: 0.7803999781608582']\n",
            "\n",
            "\n",
            "Epoch 10: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5885206460952759', 'Train frobenius: 205.33912658691406', 'Train acc: 0.8156266808509827']\n",
            "['Test loss: 0.5923453569412231', 'Test frobenius: 208.896728515625', 'Test acc: 0.8123999834060669']\n",
            "\n",
            "\n",
            "Epoch 11: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5657504796981812', 'Train frobenius: 212.3080596923828', 'Train acc: 0.8268799781799316']\n",
            "['Test loss: 0.6634190678596497', 'Test frobenius: 215.78997802734375', 'Test acc: 0.7851999998092651']\n",
            "\n",
            "\n",
            "Epoch 12: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5469933748245239', 'Train frobenius: 219.0216827392578', 'Train acc: 0.8333600163459778']\n",
            "['Test loss: 0.6134464144706726', 'Test frobenius: 222.14651489257812', 'Test acc: 0.8068000078201294']\n",
            "\n",
            "\n",
            "Epoch 13: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5331059694290161', 'Train frobenius: 225.49172973632812', 'Train acc: 0.8369066715240479']\n",
            "['Test loss: 0.6116775274276733', 'Test frobenius: 228.66709899902344', 'Test acc: 0.8064000010490417']\n",
            "\n",
            "\n",
            "Epoch 14: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5219836235046387', 'Train frobenius: 231.7968292236328', 'Train acc: 0.8420799970626831']\n",
            "['Test loss: 0.5590624213218689', 'Test frobenius: 234.9167022705078', 'Test acc: 0.8284000158309937']\n",
            "\n",
            "\n",
            "Epoch 15: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5106735229492188', 'Train frobenius: 237.86126708984375', 'Train acc: 0.8456000089645386']\n",
            "['Test loss: 0.5646620392799377', 'Test frobenius: 240.74302673339844', 'Test acc: 0.8248000144958496']\n",
            "\n",
            "\n",
            "Epoch 16: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.49951428174972534', 'Train frobenius: 243.6370849609375', 'Train acc: 0.8476799726486206']\n",
            "['Test loss: 0.5908524394035339', 'Test frobenius: 246.4822235107422', 'Test acc: 0.8159999847412109']\n",
            "\n",
            "\n",
            "Epoch 17: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.488305002450943', 'Train frobenius: 249.3145294189453', 'Train acc: 0.8542400002479553']\n",
            "['Test loss: 0.5917938351631165', 'Test frobenius: 252.06832885742188', 'Test acc: 0.8123999834060669']\n",
            "\n",
            "\n",
            "Epoch 18: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 106.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.47976329922676086', 'Train frobenius: 254.96824645996094', 'Train acc: 0.8561866879463196']\n",
            "['Test loss: 0.5486063957214355', 'Test frobenius: 257.779296875', 'Test acc: 0.8371999859809875']\n",
            "\n",
            "\n",
            "Epoch 19: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4678904414176941', 'Train frobenius: 260.3900146484375', 'Train acc: 0.8606399893760681']\n",
            "['Test loss: 0.5596871972084045', 'Test frobenius: 263.07208251953125', 'Test acc: 0.8248000144958496']\n",
            "\n",
            "\n",
            "Epoch 20: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 107.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4589526653289795', 'Train frobenius: 265.71295166015625', 'Train acc: 0.8630933165550232']\n",
            "['Test loss: 0.6037441492080688', 'Test frobenius: 268.241455078125', 'Test acc: 0.8144000172615051']\n",
            "\n",
            "\n",
            "Epoch 21: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4505407512187958', 'Train frobenius: 270.8359680175781', 'Train acc: 0.8689866662025452']\n",
            "['Test loss: 0.5775637626647949', 'Test frobenius: 273.45526123046875', 'Test acc: 0.8240000009536743']\n",
            "\n",
            "\n",
            "Epoch 22: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:11<00:00, 104.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4454289972782135', 'Train frobenius: 275.8782958984375', 'Train acc: 0.8719733357429504']\n",
            "['Test loss: 0.580893874168396', 'Test frobenius: 278.3769836425781', 'Test acc: 0.8203999996185303']\n",
            "\n",
            "\n",
            "Epoch 23: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:11<00:00, 102.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4419558644294739', 'Train frobenius: 280.7698974609375', 'Train acc: 0.8707200288772583']\n",
            "['Test loss: 0.5410348176956177', 'Test frobenius: 283.1145324707031', 'Test acc: 0.8339999914169312']\n",
            "\n",
            "\n",
            "Epoch 24: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:11<00:00, 102.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4309992790222168', 'Train frobenius: 285.4841613769531', 'Train acc: 0.8738133311271667']\n",
            "['Test loss: 0.5752373933792114', 'Test frobenius: 287.9178161621094', 'Test acc: 0.8180000185966492']\n",
            "\n",
            "\n",
            "Epoch 25: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.42354220151901245', 'Train frobenius: 290.2236328125', 'Train acc: 0.8804266452789307']\n",
            "['Test loss: 0.5495389699935913', 'Test frobenius: 292.60137939453125', 'Test acc: 0.8312000036239624']\n",
            "\n",
            "\n",
            "Epoch 26: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 106.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4137227535247803', 'Train frobenius: 294.7960205078125', 'Train acc: 0.8834133148193359']\n",
            "['Test loss: 0.5359693765640259', 'Test frobenius: 296.94927978515625', 'Test acc: 0.8331999778747559']\n",
            "\n",
            "\n",
            "Epoch 27: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:11<00:00, 106.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.41595765948295593', 'Train frobenius: 299.24932861328125', 'Train acc: 0.8807733058929443']\n",
            "['Test loss: 0.5308839082717896', 'Test frobenius: 301.4259338378906', 'Test acc: 0.828000009059906']\n",
            "\n",
            "\n",
            "Epoch 28: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.40863701701164246', 'Train frobenius: 303.6770935058594', 'Train acc: 0.8839200139045715']\n",
            "['Test loss: 0.5348280668258667', 'Test frobenius: 305.8315124511719', 'Test acc: 0.8327999711036682']\n",
            "\n",
            "\n",
            "Epoch 29: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.4027594327926636', 'Train frobenius: 307.9668273925781', 'Train acc: 0.8867999911308289']\n",
            "['Test loss: 0.5669739842414856', 'Test frobenius: 310.2506408691406', 'Test acc: 0.8240000009536743']\n",
            "\n",
            "\n",
            "Epoch 30: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 109.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3990800380706787', 'Train frobenius: 312.2816467285156', 'Train acc: 0.8884533047676086']\n",
            "['Test loss: 0.5563724040985107', 'Test frobenius: 314.5736389160156', 'Test acc: 0.8320000171661377']\n",
            "\n",
            "\n",
            "Epoch 31: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.39370933175086975', 'Train frobenius: 316.573974609375', 'Train acc: 0.8900266885757446']\n",
            "['Test loss: 0.5605283379554749', 'Test frobenius: 318.64837646484375', 'Test acc: 0.8312000036239624']\n",
            "\n",
            "\n",
            "Epoch 32: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 109.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.38167232275009155', 'Train frobenius: 320.74249267578125', 'Train acc: 0.892853319644928']\n",
            "['Test loss: 0.5510343909263611', 'Test frobenius: 322.9329833984375', 'Test acc: 0.8288000226020813']\n",
            "\n",
            "\n",
            "Epoch 33: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 110.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3808073103427887', 'Train frobenius: 324.9729309082031', 'Train acc: 0.8945066928863525']\n",
            "['Test loss: 0.5162612199783325', 'Test frobenius: 327.0044860839844', 'Test acc: 0.8416000008583069']\n",
            "\n",
            "\n",
            "Epoch 34: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3828275203704834', 'Train frobenius: 328.960693359375', 'Train acc: 0.8939999938011169']\n",
            "['Test loss: 0.5450395345687866', 'Test frobenius: 330.9981689453125', 'Test acc: 0.8235999941825867']\n",
            "\n",
            "\n",
            "Epoch 35: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 109.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3817773461341858', 'Train frobenius: 332.8697509765625', 'Train acc: 0.8921599984169006']\n",
            "['Test loss: 0.5962162613868713', 'Test frobenius: 334.7948913574219', 'Test acc: 0.8064000010490417']\n",
            "\n",
            "\n",
            "Epoch 36: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 108.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3693532943725586', 'Train frobenius: 336.7474060058594', 'Train acc: 0.8977066874504089']\n",
            "['Test loss: 0.5462318062782288', 'Test frobenius: 338.7220764160156', 'Test acc: 0.8320000171661377']\n",
            "\n",
            "\n",
            "Epoch 37: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 109.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3733711242675781', 'Train frobenius: 340.6061706542969', 'Train acc: 0.8972266912460327']\n",
            "['Test loss: 0.5286014676094055', 'Test frobenius: 342.4998779296875', 'Test acc: 0.8339999914169312']\n",
            "\n",
            "\n",
            "Epoch 38: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 109.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.3623160719871521', 'Train frobenius: 344.2376708984375', 'Train acc: 0.9017599821090698']\n",
            "['Test loss: 0.5689138770103455', 'Test frobenius: 346.0486755371094', 'Test acc: 0.8212000131607056']\n",
            "\n",
            "\n",
            "Epoch 39: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:10<00:00, 109.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.36203983426094055', 'Train frobenius: 347.9909362792969', 'Train acc: 0.899786651134491']\n",
            "['Test loss: 0.5561687350273132', 'Test frobenius: 349.8793640136719', 'Test acc: 0.8208000063896179']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myMod2 = DeepConv(tf.keras.optimizers.Adam(), L2_norm = 0.2, batch_norm = True, dropout_rate=0.1,label_smoothing=0.01)\n",
        "\n",
        "print(\"Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\")\n",
        "run(myMod2,40,save=False,config_name = f'DEEP1',augmentation=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yc1qdMV6Ni0"
      },
      "outputs": [],
      "source": [
        "run(myMod1,10,load=True,config_name = f'BASIC1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FqWcGkv9NlW",
        "outputId": "17824a42-2236-481c-8e2f-a280c0a0047f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Densely Connected Network, optimizer=SGD, learning_rate=0.001, L1 regularizer, dropout layers\n",
            "Epoch 0: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:26<00:00, 44.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 1.4987434148788452', 'Train total_frobenius_norm: 132.1035919189453', 'Train acc: 0.4668533205986023']\n",
            "['Test loss: 1.770615577697754', 'Test total_frobenius_norm: 145.5334930419922', 'Test acc: 0.38359999656677246']\n",
            "\n",
            "\n",
            "Epoch 1: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:21<00:00, 55.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 1.1346096992492676', 'Train total_frobenius_norm: 157.26976013183594', 'Train acc: 0.6113066673278809']\n",
            "['Test loss: 2.2738325595855713', 'Test total_frobenius_norm: 168.19729614257812', 'Test acc: 0.35120001435279846']\n",
            "\n",
            "\n",
            "Epoch 2: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:21<00:00, 55.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.9670271873474121', 'Train total_frobenius_norm: 177.53662109375', 'Train acc: 0.6739466786384583']\n",
            "['Test loss: 2.3326914310455322', 'Test total_frobenius_norm: 186.3407440185547', 'Test acc: 0.4047999978065491']\n",
            "\n",
            "\n",
            "Epoch 3: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:40<00:00, 28.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.8614252805709839', 'Train total_frobenius_norm: 194.89694213867188', 'Train acc: 0.7155200242996216']\n",
            "['Test loss: 2.6196374893188477', 'Test total_frobenius_norm: 202.9435577392578', 'Test acc: 0.3720000088214874']\n",
            "\n",
            "\n",
            "Epoch 4: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:21<00:00, 54.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.7868863940238953', 'Train total_frobenius_norm: 210.43519592285156', 'Train acc: 0.7448800206184387']\n",
            "['Test loss: 1.8504981994628906', 'Test total_frobenius_norm: 217.52366638183594', 'Test acc: 0.4684000015258789']\n",
            "\n",
            "\n",
            "Epoch 5: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:21<00:00, 54.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.738247811794281', 'Train total_frobenius_norm: 224.3770294189453', 'Train acc: 0.7612533569335938']\n",
            "['Test loss: 1.7900032997131348', 'Test total_frobenius_norm: 230.9181365966797', 'Test acc: 0.49000000953674316']\n",
            "\n",
            "\n",
            "Epoch 6: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:40<00:00, 28.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6879556775093079', 'Train total_frobenius_norm: 237.38070678710938', 'Train acc: 0.7815200090408325']\n",
            "['Test loss: 1.8709396123886108', 'Test total_frobenius_norm: 243.62176513671875', 'Test acc: 0.46160000562667847']\n",
            "\n",
            "\n",
            "Epoch 7: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:40<00:00, 28.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6576483249664307', 'Train total_frobenius_norm: 249.92515563964844', 'Train acc: 0.7918400168418884']\n",
            "['Test loss: 1.5486187934875488', 'Test total_frobenius_norm: 256.20703125', 'Test acc: 0.5532000064849854']\n",
            "\n",
            "\n",
            "Epoch 8: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:40<00:00, 28.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6288931369781494', 'Train total_frobenius_norm: 261.8206787109375', 'Train acc: 0.8018666505813599']\n",
            "['Test loss: 1.8987455368041992', 'Test total_frobenius_norm: 267.2981262207031', 'Test acc: 0.4575999975204468']\n",
            "\n",
            "\n",
            "Epoch 9: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:21<00:00, 55.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6021299958229065', 'Train total_frobenius_norm: 273.0610046386719', 'Train acc: 0.812613308429718']\n",
            "['Test loss: 2.320575475692749', 'Test total_frobenius_norm: 278.32366943359375', 'Test acc: 0.4643999934196472']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myMod2 = DenselyConnectedCNN(tf.keras.optimizers.Adam(learning_rate=0.001), L1_norm=0.2,batch_norm = True, dropout_rate=0.1,label_smoothing=0.01)\n",
        "\n",
        "print(\"Densely Connected Network, optimizer=SGD, learning_rate=0.001, L1 regularizer, dropout layers\")\n",
        "run(myMod2,10,save=False,config_name = f'DENS1',augmentation=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E27igxeG6x02",
        "outputId": "73690196-b598-4017-d2e1-ba794157f7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Residual Connected Network, optimizer=SGD, learning_rate=0.001, L1 regularizer, dropout layers\n",
            "Epoch 0: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:19<00:00, 59.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 1.5819414854049683', 'Train total_frobenius_norm: 143.693359375', 'Train acc: 0.4304533302783966']\n",
            "['Test loss: 2.2968664169311523', 'Test total_frobenius_norm: 157.79173278808594', 'Test acc: 0.27239999175071716']\n",
            "\n",
            "\n",
            "Epoch 1: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 1.151503324508667', 'Train total_frobenius_norm: 170.3424530029297', 'Train acc: 0.6025333404541016']\n",
            "['Test loss: 1.4729399681091309', 'Test total_frobenius_norm: 181.98414611816406', 'Test acc: 0.5044000148773193']\n",
            "\n",
            "\n",
            "Epoch 2: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.9594497680664062', 'Train total_frobenius_norm: 192.2401885986328', 'Train acc: 0.6773333549499512']\n",
            "['Test loss: 1.3764421939849854', 'Test total_frobenius_norm: 202.44134521484375', 'Test acc: 0.5368000268936157']\n",
            "\n",
            "\n",
            "Epoch 3: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.8404232263565063', 'Train total_frobenius_norm: 211.59786987304688', 'Train acc: 0.7221599817276001']\n",
            "['Test loss: 1.329484462738037', 'Test total_frobenius_norm: 220.55667114257812', 'Test acc: 0.5544000267982483']\n",
            "\n",
            "\n",
            "Epoch 4: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.7672138214111328', 'Train total_frobenius_norm: 228.84271240234375', 'Train acc: 0.7515199780464172']\n",
            "['Test loss: 1.1920722723007202', 'Test total_frobenius_norm: 236.96311950683594', 'Test acc: 0.6028000116348267']\n",
            "\n",
            "\n",
            "Epoch 5: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.7111297845840454', 'Train total_frobenius_norm: 244.51547241210938', 'Train acc: 0.7706133127212524']\n",
            "['Test loss: 1.4229668378829956', 'Test total_frobenius_norm: 251.94906616210938', 'Test acc: 0.5555999875068665']\n",
            "\n",
            "\n",
            "Epoch 6: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.6691812872886658', 'Train total_frobenius_norm: 259.2087097167969', 'Train acc: 0.7870399951934814']\n",
            "['Test loss: 1.234609842300415', 'Test total_frobenius_norm: 266.5299072265625', 'Test acc: 0.6007999777793884']\n",
            "\n",
            "\n",
            "Epoch 7: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:14<00:00, 78.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.630689799785614', 'Train total_frobenius_norm: 273.60333251953125', 'Train acc: 0.8010133504867554']\n",
            "['Test loss: 1.3714368343353271', 'Test total_frobenius_norm: 280.58990478515625', 'Test acc: 0.553600013256073']\n",
            "\n",
            "\n",
            "Epoch 8: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:15<00:00, 78.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5947387218475342', 'Train total_frobenius_norm: 287.42047119140625', 'Train acc: 0.8139733076095581']\n",
            "['Test loss: 1.214688777923584', 'Test total_frobenius_norm: 293.94598388671875', 'Test acc: 0.5971999764442444']\n",
            "\n",
            "\n",
            "Epoch 9: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1172/1172 [00:20<00:00, 57.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Train loss: 0.5629902482032776', 'Train total_frobenius_norm: 300.5791931152344', 'Train acc: 0.8276000022888184']\n",
            "['Test loss: 1.08224356174469', 'Test total_frobenius_norm: 306.87158203125', 'Test acc: 0.6488000154495239']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "myMod3 = ResidualConnectedCNN(tf.keras.optimizers.Adam(),batch_norm = True, dropout_rate=0.1,label_smoothing=0.01)\n",
        "\n",
        "\n",
        "print(\"Residual Connected Network, optimizer=SGD, learning_rate=0.001, L1 regularizer, dropout layers\")\n",
        "run(myMod3,10,save=False,config_name = f'RES1',augmentation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-3s6mae45Sr"
      },
      "source": [
        "**Optimizaiton techniques:**\n",
        "<br> **Option 1: Regularizer L1**\n",
        "<br> Having a lot of features, we also have a large number of weights which makes the model likely to overfit. Adding a regularizer, the weights have less impact on the loss function. There are different ways to apply regularization. Here we used the L1 regularization:\n",
        "1. Have a look at the layers and find those that have the most parameters \n",
        "2. (tf.keras.regularizer.L1) where penalties are added to the loss function (which already has a very low loss value) to minimize parameter magnitudes. It penalizes the sum of absolute values for the weights for example. By doing so, some features will be mutliplied by weights with value 0 making them not influential. \n",
        "\n",
        "<br> **Option 2: Regularizer L2**\n",
        "<br> Having a lot of features, we also have a large number of weights which makes the model likely to overfit. Adding a regularizer, the weights have less impact on the loss function. There are different ways to apply regularization. Here we used the L2 regularization:\n",
        "1. Have a look at the layers and find those that have the most parameters \n",
        "2. (tf.keras.regularizer.L2) where penalties are added to the loss function (which already has a very low loss value) to minimize parameter magnitudes. The sum of all parameters squared are penalized, making them smaller but not 0. \n",
        "\n",
        "<br> L1 and L2 both penalize during backpropagation. They add a penalty (by adding the regularization term to the loss function) for the parameter to the derivative while backpropagating through the network, making the weights a little smaller with each update. \n",
        "\n",
        "<br> **Option 3: Dropout**\n",
        "<br> tf.keras.layers.Dropout(0.2) - the layer will be dropped with a probability of 0.2 during training. \n",
        "<br> In contrast to L1 and L2 which modify the loss of the network, dropout modifies the network itself. By dropping a input value based on the highest probability the network will not favor any feature. In parallel different network architectures are trained. Those nodes that are not dropped take more responsibility for that specific layer and the overall process will be more noisy with dropping out layers at random. Overall we also reduce the complexity of our neural network, as it becomes smaller.\n",
        "\n",
        "\n",
        "<br> **Option 4: Data augmentation**\n",
        "<br> apply data augmentation in the pipline of preprocessing by rescaling and resizing the input values (change brightness - tf.image.stateless_random_brightness, contrast - tf.image.stateless_random_contrast, flip the images - tf.image.stateless_random_flip_left_right, saturation - tf.image.stateless_random_saturation)\n",
        "<br> The goal of data augmentation is to add random transformation to the data to increase its diversity. By adding more data (by augmenting the data) you get more diverse versions of the original data and the variance will decrease. It will help to prevent overfitting but is even good in general to increase the performance.\n",
        "\n",
        "<br> **Option 5: Label smoothing** \n",
        "<br> When having hard targets, which would be one-hot targets for example, our model can become too confident and we face a problem - overfitting. \n",
        "To make the model more general again, we can apply label smoothing with soft targets instead. Probabilities are assigned to the classes we can predict. Hard targets show no uncertainty as they are either 0 or 1 for the prediction of a class. So the idea is to reduce the confidence on the prediction of a label. How much we lower the confidence is determined by a value alpha (i.e alpha = 0.1, the confidence decreases from 1 to 0.9).\n",
        "\n",
        "<br> **Option 6: Early stopping** \n",
        "<br> After some iterations we start to overfit our model and the error on the test data increases again. The idea of early stopping is to stop at that specific point where the error on the test data increases again. It would be great to apply additional techniques to make sure that the parameters are also well chosen (i.e add a decreasing learning rate, add weight regularization such as Regularizer L1 or L2).\n",
        "\n",
        "Looking at our different techniques and their results, **xxx** works best by doing **xxx**. \n",
        "\n",
        "<br> **Option 7: Batch Normalization** \n",
        "<br> Batch normalization helps with overfitting by stabilizing the network during training. It normalizes the input before it is fed into the next hidden layer.\n",
        "What does normalization do in general? We might have input values that are in very different scales, which can result in larger features that need larger updates to their weights compared to smaller features and hence to an oscillating loss landscape. Normalization solves this problem by bringing the values into a scaled range of values. \n",
        "Batch normalization is done inside the network where different activation vectors exist for each feature. Separately the mean and variance are calculated and the values normalized. Then they are shifted to a different mean and variance to get the best predictions.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}