{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwuExdWQNAFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb703c6-fb8a-43f7-ebdf-65a06631c830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "@authors: faurand, chardes, ehagensieker\n",
        "\"\"\"\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import datetime\n",
        "import tqdm\n",
        "import pprint\n",
        "\n",
        "# in a notebook, load the tensorboard extension, not needed for scripts\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], as_supervised=True)"
      ],
      "metadata": {
        "id": "_Jp8ZwI0O9Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Warning: this version simplifies the procedure by extracting only the targets!\n",
        "# ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
        "def cumsum_dataset(ds, seq_len):\n",
        "  #only get the targets, to keep this demonstration simple (and force students to understand the code if they are using it by rewriting it respectively)\n",
        "  ds = ds.map(lambda x, t: tf.cast(t, dtype=tf.dtypes.int32))\n",
        "  # use window to create subsequences. This means ds is not a dataset of datasets, i.e. every single entry in the dataset is itself a small tf.data.Dataset object with seq_len many entries!\n",
        "  ds = ds.window(seq_len)\n",
        "  #make sure to check tf.data.Dataset.scan() to understand how this works!\n",
        "  def alternating_scan_function(state, elem):\n",
        "    #state is allways the sign to use!\n",
        "    old_sign = state\n",
        "    #just flip the sign for every element\n",
        "    new_sign = old_sign*-1\n",
        "    #elem is just the target of the element. We need to apply the appropriate sign to it!\n",
        "    signed_target = elem*old_sign\n",
        "    #we need to return a tuple for the scan function: The new state and the output element\n",
        "    out_elem = signed_target\n",
        "    new_state = new_sign\n",
        "    return new_state, out_elem\n",
        "  #we now want to apply this function via scanning, resulting in a dataset where the signs are alternating\n",
        "  #remember we have a dataset, where each element is a sub dataset due to the windowing!\n",
        "  ds = ds.map(lambda sub_ds: sub_ds.scan(initial_state=1, scan_func=alternating_scan_function))\n",
        "  #now we need a scanning function which implements a cumulative sum, very similar to the cumsum used above\n",
        "  def scan_cum_sum_function(state, elem):\n",
        "    #state is the sum up the the current element, element is the new digit to add to it\n",
        "    sum_including_this_elem = state+elem\n",
        "    #both the element at this position and the returned state should just be sum up to this element, saved in sum_including_this_elem\n",
        "    return sum_including_this_elem, sum_including_this_elem\n",
        "  #again we want to apply this to the subdatasets via scan, with a starting state of 0 (sum before summing is zero...)\n",
        "  ds = ds.map(lambda sub_dataset: sub_dataset.scan(initial_state=0, scan_func=scan_cum_sum_function))\n",
        "  #finally we need to create a single element from everything in the subdataset\n",
        "  ds = ds.map(lambda sub_dataset: sub_dataset.batch(seq_len).get_single_element())\n",
        "\n",
        "  \n",
        "  return ds\n",
        "\n",
        "# SEQ_LEN = 10\n",
        "# for elem in ds_train.apply(lambda dataset: cumsum_dataset(dataset, SEQ_LEN)).take(10):\n",
        "#   print(elem)\n"
      ],
      "metadata": {
        "id": "8NuqAUfPVuBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_target_fnc(ds, sequence_len):\n",
        "\n",
        "  l = list()\n",
        "  for i, elem in enumerate(ds):\n",
        "    if (i % sequence_len) == 0:\n",
        "      l.append(int(elem[1]))\n",
        "    else:\n",
        "      if (i % 2) == 0:\n",
        "        l.append(int(l[i-1] + elem[1]))\n",
        "      else:\n",
        "        l.append(int(l[i-1] - elem[1]))\n",
        "  return l\n",
        "     "
      ],
      "metadata": {
        "id": "9qPYOugeNlYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data,batch_size = 32, seq_length = 4, augmentation = None): \n",
        "    '''\n",
        "    prepare the dataset to have one-hot-vectors and values between -1 and 1\n",
        "    '''\n",
        "    data = data.map(lambda img, target: (img[0], target))\n",
        "    data = data.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
        "    #convert data from uint8 to float32\n",
        "    data = data.map(lambda img, target: (tf.cast(img, tf.float32), tf.cast(target, tf.int32)))\n",
        "    #normalization to have input of range [-1,1]\n",
        "    data = data.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "\n",
        "\n",
        "    data = data.batch(seq_length).cache().shuffle(1028).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "    return data \n",
        "\n",
        "seq_len = 4\n",
        "\n",
        "# train_targets = tf.data.Dataset.from_tensor_slices(cumsum_dataset(train_ds, 4))\n",
        "# test_targets = tf.data.Dataset.from_tensor_slices(cumsum_dataset(test_ds, 4))\n",
        "\n",
        "\n",
        "#Zip up datasets and new targets\n",
        "# train_ds = tf.data.Dataset.zip((train_ds, train_targets))\n",
        "# test_ds = tf.data.Dataset.zip((test_ds, test_targets))\n",
        "\n",
        "\n",
        "#Prepare the new datasets\n",
        "# train = prepare_data(train_ds)\n",
        "# test = prepare_data(test_ds)\n",
        "\n",
        "# train_targets = cumsum_dataset(train, seq_len)\n",
        "# test_targets = cumsum_dataset(test, seq_len)\n"
      ],
      "metadata": {
        "id": "jTpxf_bFuOfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBu7JQOX65EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in train.take(1):\n",
        "  #print(train_ds.take(1))\n",
        "  print(e[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRSAFYvf65lg",
        "outputId": "7a9cc0fb-4a1b-4876-9922-f9eb812eb83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1 0 6 7]\n",
            " [2 7 9 6]\n",
            " [2 7 1 6]\n",
            " [1 5 5 4]\n",
            " [4 2 1 5]\n",
            " [1 0 4 9]\n",
            " [7 3 3 5]\n",
            " [0 6 9 8]\n",
            " [4 0 7 5]\n",
            " [1 7 9 0]\n",
            " [1 8 6 6]\n",
            " [8 1 8 7]\n",
            " [7 4 8 5]\n",
            " [7 7 3 3]\n",
            " [6 8 3 6]\n",
            " [7 0 2 1]\n",
            " [7 5 7 4]\n",
            " [0 6 6 9]\n",
            " [5 9 6 6]\n",
            " [9 9 1 7]\n",
            " [9 1 3 6]\n",
            " [1 5 8 3]\n",
            " [2 9 4 6]\n",
            " [5 0 2 5]\n",
            " [7 5 9 6]\n",
            " [3 4 6 3]\n",
            " [2 3 0 0]\n",
            " [0 6 2 4]\n",
            " [5 0 5 7]\n",
            " [6 0 0 9]\n",
            " [2 1 3 1]\n",
            " [6 8 7 7]], shape=(32, 4), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "class BasicCNN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        self.layer=tf.keras.layers.Conv2D(32, 3, padding = 'same', activation=\"relu\")\n",
        "        \n",
        "        self.output_basic = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAvgPool2D())\n",
        "\n",
        "    def call(self, sequence, training=False):\n",
        "        # t=0 - just zeros in first time step \n",
        "        # first time step with all features and add the initial state \n",
        "        layer_state_t0 = self.layer(sequence[:,0,:] + tf.zeros_like(sequence[:,0,:]))\n",
        "        #apply dense layer again with the second time step \n",
        "        layer_state_t1 = self.layer(layer_state_t0 + sequence[:,1,:])\n",
        "        layer_state_t2 = self.layer(layer_state_t1 + sequence[:,2,:])\n",
        "        layer_state_t3 = self.layer(layer_state_t2 + sequence[:,3,:])\n",
        "        \n",
        "        return self.output_layer(layer_state_t3)"
      ],
      "metadata": {
        "id": "KHUtJhPibIau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "class LSTMWrapper(tf.keras.layers.RNN):\n",
        "    \n",
        "    def __init__(self, AbstractRNNCelllayer, return_sequences = True):\n",
        "        super(LSTMWrapper).__init__()\n",
        "        \n",
        "        self.cell = AbstractRNNCelllayer\n",
        "\n",
        "    def call(self, seq_len, data, hs, cs):\n",
        "        out = []\n",
        "        hidden_state = hs\n",
        "        cell_state = cs\n",
        "        for t in range(seq_len):\n",
        "            hidden_state, cell_state = self.cell(data[:,t,:], hidden_state,cell_state)\n",
        "            out.append(hidden_state)\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def zeros(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.cell.units), dtype=tf.float32)"
      ],
      "metadata": {
        "id": "T8ftkm_ubxl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "class AbstractRNNCelllayer(tf.keras.layers.AbstractRNNCell):\n",
        "# can be arbitrarily complex\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(AbstractRNNCelllayer).__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "\n",
        "        self.forget_layer = tf.keras.layers.Dense(units,  \n",
        "                                                   activation=\"sigmoid\",\n",
        "                                                  bias_initializer='ones')\n",
        "        self.input_layer = tf.keras.layers.Dense(units, \n",
        "                                                 activation=\"sigmoid\")\n",
        "\n",
        "        self.candidates_layer = tf.keras.layers.Dense(units, \n",
        "                                                 activation=\"tanh\")\n",
        "        \n",
        "        self.output_layer = tf.keras.layers.Dense(units, \n",
        "                                                 activation=\"sigmoid\")\n",
        "    \n",
        "    @property\n",
        "    def state_size(self):\n",
        "      #dimensionality of the RNN state\n",
        "      return [tf.TensorShape([self.recurrent_units_1]), \n",
        "              tf.TensorShape([self.recurrent_units_2])]\n",
        "    @property\n",
        "    def output_size(self):\n",
        "      # list containing final output \n",
        "      return [tf.TensorShape([self.recurrent_units_2])]\n",
        "    \n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
        "      # states are then passed to the call method\n",
        "      return [tf.zeros([self.recurrent_units_1]), \n",
        "                tf.zeros([self.recurrent_units_2])]\n",
        "\n",
        "    def call(self, inputs, hidden_state, cell_state):\n",
        "        input = tf.concat([input, hidden_state], axis = 1)\n",
        "\n",
        "        forget_gate = self.forget_layer(input)\n",
        "\n",
        "        passed_forget = tf.math.multiply(forget_gate, hidden_state)\n",
        "\n",
        "        input_gate = self.input_layer(input)\n",
        "        candidates = self.candidates_layer(input)\n",
        "        new_cell_state = tf.math.add(passed_forget, tf.math.multiply(input_gate, candidates))\n",
        "\n",
        "        output_gate = self.output_layer(input)\n",
        "        new_hidden_state = np.multiply(output_gate, tf.nn.tanh(new_cell_state))\n",
        "\n",
        "        return [new_hidden_state, new_cell_state]"
      ],
      "metadata": {
        "id": "Oxs2B74raWEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "class RNN_Model(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, seq_len = 4, optimizer = tf.keras.optimizers.Adam()):\n",
        "        super(RNN_Model, self).__init__()\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "        self.input_ly = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "        self.LSTM =  LSTMWrapper(AbstractRNNCelllayer(2))\n",
        "        self.output_ly = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "\n",
        "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                             tf.keras.metrics.BinaryAccuracy()]\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return self.metrics_list\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        for metric in self.metrics:\n",
        "            metric.reset_state()\n",
        "\n",
        "    def call(self,data):\n",
        "        x = self.input_ly(data)\n",
        "        x= self.LSTM(x,self.LSTM.zeros(8),self.LSTM.zeros(8))\n",
        "        return [self.output_ly(val) for val in x]\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, train_data):\n",
        "        x, target = train_data\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self(x, training = True)\n",
        "            loss = self.loss_function(target, output)\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(target, output)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}\n",
        "\n",
        "    @tf.function\n",
        "    def test(self, test_data):\n",
        "\n",
        "        x, target = test_data\n",
        "        output = self(x, training=False)\n",
        "        loss = self.loss_function(target, output)\n",
        "                \n",
        "        self.metrics[0].update_state(loss)\n",
        "        self.metrics[1].update_state(target, output)\n",
        "        \n",
        "        return {m.name : m.result() for m in self.metrics}  \n"
      ],
      "metadata": {
        "id": "VlfuwY-HcCgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer): \n",
        "    '''\n",
        "    create the training loop where the model is trained on the train dataset and afterwards tested on the validation dataset\n",
        "    '''\n",
        "    hist = {\"train_loss\":[],\"train_acc\":[],\"test_loss\":[], \"test_acc\":[]}\n",
        "    #iterate over given amount of epochs\n",
        "    for epoch in range(epochs): \n",
        "        print(f\"Epoch {epoch}: \")\n",
        "\n",
        "        #train on all batches of the training data\n",
        "        for data in tqdm.tqdm(train_ds, position = 0, leave = True):\n",
        "            metrics = model.train_step(data)\n",
        "\n",
        "            with train_summary_writer.as_default(): \n",
        "                for metric in model.metrics: \n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics and add to history element\n",
        "        for key, value in metrics.items():\n",
        "            hist[f\"train_{key}\"].append(value.numpy())\n",
        "            print(f\"train_{key}: {value.numpy()}\")\n",
        "\n",
        "        #reset metric \n",
        "        model.reset_metrics()\n",
        "\n",
        "        #evaluation on validation set\n",
        "        for data in val_ds:\n",
        "            metrics = model.test_step(data)\n",
        "\n",
        "            with val_summary_writer.as_default():\n",
        "                for metric in model.metrics:\n",
        "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "\n",
        "        # print the metrics and add to history element\n",
        "        for key, value in metrics.items():\n",
        "            hist[f\"test_{key}\"].append(value.numpy())\n",
        "            print(f\"test_{key}: {value.numpy()}\")\n",
        "\n",
        "        #reset metric\n",
        "        model.reset_metric()\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    return hist"
      ],
      "metadata": {
        "id": "o3Vw2wNSOa8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_summary_writers(config_name):\n",
        "  '''\n",
        "  create the summary writer to have access to the metrics of the model \n",
        "  '''\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "  train_log_path = f\"logs/submission/{config_name}/{current_time}/train\"\n",
        "  val_log_path = f\"logs/submission/{config_name}/{current_time}/val\"\n",
        "\n",
        "  # log writer\n",
        "  train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "  val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "    \n",
        "  return train_summary_writer, val_summary_writer\n"
      ],
      "metadata": {
        "id": "YJOjZE4GTLaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run(model,num_epochs=15,save=True,load=False,config_name = f'RUN'):\n",
        "  '''\n",
        "  run the model and specify the number of epochs, batch_size and prepare the training and testing data\n",
        "  '''\n",
        "\n",
        "  ### Hyperparameters\n",
        "  num_epochs = num_epochs\n",
        "  batch_size = 32\n",
        "\n",
        "  \n",
        "  \n",
        "  train_summary_writer, val_summary_writer = create_summary_writers(config_name)\n",
        "\n",
        "  train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "  train_targets = tf.data.Dataset.from_tensor_slices(new_target_fnc(train_ds, 4))\n",
        "  test_targets = tf.data.Dataset.from_tensor_slices(new_target_fnc(test_ds, 4))\n",
        "\n",
        "  train_ds = tf.data.Dataset.zip((train_ds, train_targets))\n",
        "  test_ds = tf.data.Dataset.zip((test_ds, test_targets))\n",
        "  # train_ds = cumsum_dataset(train_targets, 4)\n",
        "  # test_ds = cumsum_dataset(test_targets, 4)\n",
        "\n",
        "  train = prepare_data(train_ds)  \n",
        "  test = prepare_data(test_ds)\n",
        "\n",
        "  if load:\n",
        "    model.load_weights(f\"saved_model_{config_name}\");\n",
        "\n",
        "  hist = training_loop(model,train,test,num_epochs,train_summary_writer, val_summary_writer)\n",
        "\n",
        "  if save:\n",
        "    model.save_weights(f\"saved_model_{config_name}\", save_format=\"tf\")\n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "  fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
        "\n",
        "  ax[0].plot(hist['train_acc'], label='train accuracy')\n",
        "  ax[0].plot(hist['test_acc'], label='test accuracy')\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(hist['train_loss'], label='train loss')\n",
        "  ax[1].plot(hist['test_loss'], label='test loss')\n",
        "  ax[1].legend();"
      ],
      "metadata": {
        "id": "NvlAuHyDTUcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myMod2 = RNN_Model()\n",
        "print(\"Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\")\n",
        "run(myMod2)"
      ],
      "metadata": {
        "id": "zU2uckevTWJ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "49bfeb54-8c1a-4c95-bac0-a23821e47c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\n",
            "Epoch 0: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-415-16991e09099a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmyMod2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Basic Convolutional Network, optimizer=Adam, L1 regularizer, dropout layers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyMod2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-414-f9f288718152>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, num_epochs, save, load, config_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"saved_model_{config_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-412-a90f4a454bba>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#train on all batches of the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file4l2xtn9f.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_clear_losses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2170\u001b[0;31m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_keras_tensor_symbolic_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"<ipython-input-355-e4f2cff3ed9b>\", line 35, in train_step  *\n        output = self(x, training = True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py\", line 2170, in _clear_losses\n        layer._thread_local._eager_losses = []\n\n    AttributeError: 'LSTMWrapper' object has no attribute '_thread_local'\n"
          ]
        }
      ]
    }
  ]
}